{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus 예시:\n",
      "   doc_id                                               text\n",
      "0       0  The U.S. Social Security Administration (SSA),...\n",
      "1       1  Arnold Alois Schwarzenegger (/ˈʃwɔːrtsənˌɛɡər/...\n",
      "2       2  In 2006, the Sister City Program of the City o...\n",
      "3       3  By 1840, the Market Hall and Sheds, where fres...\n",
      "4       4  Some commentators have defined reverse discrim...\n",
      "\n",
      "QA 쌍 예시:\n",
      "                                            question  \\\n",
      "0  To whom did the Virgin Mary allegedly appear i...   \n",
      "1  What is in front of the Notre Dame Main Building?   \n",
      "2  The Basilica of the Sacred heart at Notre Dame...   \n",
      "3                  What is the Grotto at Notre Dame?   \n",
      "4  What sits on top of the Main Building at Notre...   \n",
      "\n",
      "                                    answer  doc_id  \\\n",
      "0               Saint Bernadette Soubirous   14556   \n",
      "1                a copper statue of Christ   14556   \n",
      "2                        the Main Building   14556   \n",
      "3  a Marian place of prayer and reflection   14556   \n",
      "4       a golden statue of the Virgin Mary   14556   \n",
      "\n",
      "                                             context  \n",
      "0  Architecturally, the school has a Catholic cha...  \n",
      "1  Architecturally, the school has a Catholic cha...  \n",
      "2  Architecturally, the school has a Catholic cha...  \n",
      "3  Architecturally, the school has a Catholic cha...  \n",
      "4  Architecturally, the school has a Catholic cha...  \n"
     ]
    }
   ],
   "source": [
    "# 1. SQuAD 데이터 로드 (훈련 세트 기준, validation도 가능)\n",
    "dataset = load_dataset(\"squad\", split=\"train\")\n",
    "\n",
    "# 2. 고유한 context 문서 집합 생성 (retrieval corpus로 사용)\n",
    "unique_contexts = list(set(dataset[\"context\"]))\n",
    "corpus_df = pd.DataFrame({\"doc_id\": list(range(len(unique_contexts))), \"text\": unique_contexts})\n",
    "\n",
    "# 3. QA 쌍 구성 (질문, 정답, 해당 문서)\n",
    "qa_data = []\n",
    "context_to_id = {context: idx for idx, context in enumerate(unique_contexts)}\n",
    "\n",
    "for item in dataset:\n",
    "    question = item[\"question\"]\n",
    "    answer = item[\"answers\"][\"text\"][0] if item[\"answers\"][\"text\"] else \"\"\n",
    "    context = item[\"context\"]\n",
    "    doc_id = context_to_id[context]\n",
    "    qa_data.append({\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"doc_id\": doc_id,\n",
    "        \"context\": context\n",
    "    })\n",
    "\n",
    "qa_pairs = pd.DataFrame(qa_data)\n",
    "\n",
    "# 4. 결과 미리 보기\n",
    "print(\"Corpus 예시:\")\n",
    "print(corpus_df.head())\n",
    "\n",
    "print(\"\\nQA 쌍 예시:\")\n",
    "print(qa_pairs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ JSON 파일 저장 완료:\n",
      "- squad_rag_corpus.json\n",
      "- squad_rag_qa_pairs.json\n"
     ]
    }
   ],
   "source": [
    "# 1. corpus 저장 (Retrieval 문서들)\n",
    "corpus_records = corpus_df.to_dict(orient=\"records\")\n",
    "with open(\"dataset/squad_rag_corpus2.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(corpus_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# 2. QA 쌍 저장 (질문-정답-문서 매핑)\n",
    "qa_records = qa_pairs.to_dict(orient=\"records\")\n",
    "with open(\"dataset/squad_rag_qa_pairs2.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(qa_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ JSON 파일 저장 완료:\")\n",
    "print(\"- squad_rag_corpus.json\")\n",
    "print(\"- squad_rag_qa_pairs.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save embedded vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aix7101/anaconda3/envs/j_project/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
      "/home/aix7101/anaconda3/envs/j_project/lib/python3.9/site-packages/transformers/modeling_utils.py:463: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. Load DPR model and tokenizer (use multi-qa)\n",
    "ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "ctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "\n",
    "q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "q_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DPRQuestionEncoder(\n",
       "  (question_encoder): DPREncoder(\n",
       "    (bert_model): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx_encoder.eval()\n",
    "q_encoder.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ctx_encoder.to(device)\n",
    "q_encoder.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 문서 임베딩 생성\n",
    "ctx_embeddings = []\n",
    "for doc in tqdm(corpus_df[\"text\"], desc=\"Encoding contexts\"):\n",
    "    inputs = ctx_tokenizer(doc, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}  # 입력도 GPU로 이동\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emb = ctx_encoder(**inputs).pooler_output[0].cpu().numpy()  # 결과만 다시 CPU로\n",
    "    ctx_embeddings.append(emb)\n",
    "\n",
    "ctx_embeddings = np.stack(ctx_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding questions: 100%|██████████| 87599/87599 [14:09<00:00, 103.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# 3. 질문 임베딩 생성\n",
    "q_embeddings = []\n",
    "for q in tqdm(qa_pairs[\"question\"], desc=\"Encoding questions\"):\n",
    "    inputs = q_tokenizer(q, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}  # ✅ 입력도 GPU로 이동\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emb = q_encoder(**inputs).pooler_output[0].cpu().numpy()  # ✅ 결과만 다시 CPU로 이동\n",
    "    q_embeddings.append(emb)\n",
    "\n",
    "q_embeddings = np.stack(q_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Context embeddings saved to: /mnt/aix7101/jeong/aix_project/dpr_ctx_embeddings2.npy\n",
      "✅ Question embeddings saved to: /mnt/aix7101/jeong/aix_project/dpr_q_embeddings2.npy\n"
     ]
    }
   ],
   "source": [
    "# 4. 저장\n",
    "embedding_dir = \"/mnt/aix7101/jeong/aix_project\"\n",
    "if not os.path.exists(embedding_dir):\n",
    "    os.makedirs(embedding_dir)\n",
    "    print(f\"📁 Created directory: {embedding_dir}\")\n",
    "\n",
    "ctx_path = os.path.join(embedding_dir, \"dpr_ctx_embeddings2.npy\")\n",
    "q_path = os.path.join(embedding_dir, \"dpr_q_embeddings2.npy\")\n",
    "\n",
    "np.save(ctx_path, ctx_embeddings)\n",
    "np.save(q_path, q_embeddings)\n",
    "\n",
    "print(f\"✅ Context embeddings saved to: {ctx_path}\")\n",
    "print(f\"✅ Question embeddings saved to: {q_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize  # 문장 단위로 분리\n",
    "\n",
    "ctx_sentence_embeddings = []\n",
    "\n",
    "for doc in tqdm(corpus_df[\"text\"], desc=\"Encoding multi-sentence contexts\"):\n",
    "    # 1. 문장 단위로 나누기\n",
    "    sentences = sent_tokenize(doc)\n",
    "    \n",
    "    doc_embeddings = []\n",
    "    for sent in sentences:\n",
    "        inputs = ctx_tokenizer(\n",
    "            sent,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=128  # 문장 기준이라 길이 줄여도 OK\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            emb = ctx_encoder(**inputs).pooler_output[0].cpu().numpy()\n",
    "        doc_embeddings.append(emb)\n",
    "    \n",
    "    # 문서에 속한 문장 벡터들을 하나의 array로 (num_sents_in_doc, dim)\n",
    "    doc_embeddings = np.stack(doc_embeddings)\n",
    "    ctx_sentence_embeddings.append(doc_embeddings)\n",
    "\n",
    "# ⚠️ 문서마다 문장 수가 달라 padding이 필요할 수 있음\n",
    "# → 3D array로 만들기 위해 패딩 (optional)\n",
    "max_len = max(e.shape[0] for e in ctx_sentence_embeddings)\n",
    "dim = ctx_sentence_embeddings[0].shape[1]\n",
    "\n",
    "# zero-padding\n",
    "padded_embeddings = np.zeros((len(ctx_sentence_embeddings), max_len, dim))\n",
    "for i, emb in enumerate(ctx_sentence_embeddings):\n",
    "    padded_embeddings[i, :emb.shape[0], :] = emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 저장\n",
    "embedding_dir = \"/mnt/aix7101/jeong/aix_project\"\n",
    "if not os.path.exists(embedding_dir):\n",
    "    os.makedirs(embedding_dir)\n",
    "    print(f\"📁 Created directory: {embedding_dir}\")\n",
    "\n",
    "sentence_ctx_path = os.path.join(embedding_dir, \"dpr_m_ctx_embeddings2.npy\")\n",
    "\n",
    "np.save(sentence_ctx_path, ctx_sentence_embeddings)\n",
    "\n",
    "print(f\"✅ Context Sentence embeddings saved to: {sentence_ctx_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def compute_bm25_recall(qa_pairs: pd.DataFrame, corpus_df: pd.DataFrame, k: int = 5) -> float:\n",
    "    \"\"\"\n",
    "    BM25 기반 Recall@k 계산 함수\n",
    "    \n",
    "    Args:\n",
    "        qa_pairs (pd.DataFrame): 질문-정답 쌍이 포함된 데이터프레임 (columns: ['question', 'answer', 'doc_id'])\n",
    "        corpus_df (pd.DataFrame): 문서 집합 (columns: ['doc_id', 'text'])\n",
    "        k (int): top-k 문서 중 정답이 포함되는지 평가할 k 값\n",
    "        \n",
    "    Returns:\n",
    "        float: Recall@k\n",
    "    \"\"\"\n",
    "    # 1. 토크나이즈된 문서 리스트 생성\n",
    "    tokenized_corpus = [doc.split() for doc in corpus_df[\"text\"]]\n",
    "    \n",
    "    # 2. BM25 인덱스 구성\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    \n",
    "    hit_count = 0\n",
    "\n",
    "    # 3. 각 질문에 대해 BM25 top-k 문서 검색\n",
    "    for _, row in tqdm(qa_pairs.iterrows(), total=len(qa_pairs), desc=\"Evaluating BM25 Recall@K\"):\n",
    "        question = row[\"question\"]\n",
    "        gt_doc_id = row[\"doc_id\"]\n",
    "\n",
    "        tokenized_query = question.split()\n",
    "        scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "        # 상위 k개의 문서 인덱스 추출\n",
    "        topk_indices = np.argsort(scores)[::-1][:k]\n",
    "        topk_doc_ids = corpus_df.iloc[topk_indices][\"doc_id\"].tolist()\n",
    "\n",
    "        if gt_doc_id in topk_doc_ids:\n",
    "            hit_count += 1\n",
    "\n",
    "    recall_at_k = hit_count / len(qa_pairs)\n",
    "    print(f\"📌 BM25 Recall@{k}: {recall_at_k:.4f}\")\n",
    "    return recall_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BM25 Recall@K:  31%|███       | 27077/87599 [17:38<47:09, 21.39it/s]  "
     ]
    }
   ],
   "source": [
    "recall_bm25 = compute_bm25_recall(qa_pairs, corpus_df, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dpr_recall(qa_pairs, corpus_df, ctx_emb_path, q_emb_path, k=5):\n",
    "    \"\"\"\n",
    "    저장된 임베딩 파일을 기반으로 top-k 문서 중 정답 문서가 포함되는 비율(Recall@k)을 계산합니다.\n",
    "    \n",
    "    Args:\n",
    "        qa_pairs (pd.DataFrame): 질문-정답 쌍이 포함된 데이터프레임 (columns: ['question', 'answer', 'doc_id'])\n",
    "        corpus_df (pd.DataFrame): 문서 집합 (columns: ['doc_id', 'text'])\n",
    "        ctx_emb_path (str): 문서 임베딩이 저장된 .npy 경로\n",
    "        q_emb_path (str): 질문 임베딩이 저장된 .npy 경로\n",
    "        k (int): top-k 문서 중 정답이 포함되는지 평가할 k 값\n",
    "        \n",
    "    Returns:\n",
    "        float: Recall@k\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 임베딩 로드\n",
    "    ctx_embeddings = np.load(ctx_emb_path)\n",
    "    q_embeddings = np.load(q_emb_path)\n",
    "\n",
    "    assert len(q_embeddings) == len(qa_pairs), \"❗ 질문 임베딩 수와 QA 쌍 수가 일치하지 않습니다.\"\n",
    "\n",
    "    hit_count = 0\n",
    "\n",
    "    # 2. 각 질문에 대해 유사한 top-k 문서 검색\n",
    "    for idx, row in tqdm(qa_pairs.iterrows(), total=len(qa_pairs), desc=\"Evaluating Recall@K\"):\n",
    "        gt_doc_id = row[\"doc_id\"]\n",
    "        q_emb = q_embeddings[idx]\n",
    "\n",
    "        # 문서들과의 유사도 (cosine 유사도 대신 dot-product 사용)\n",
    "        scores = np.dot(ctx_embeddings, q_emb)\n",
    "\n",
    "        # top-k 인덱스\n",
    "        topk_indices = np.argsort(scores)[::-1][:k]\n",
    "        topk_doc_ids = corpus_df.iloc[topk_indices][\"doc_id\"].tolist()\n",
    "\n",
    "        # 정답 문서가 top-k에 포함되는지 확인\n",
    "        if gt_doc_id in topk_doc_ids:\n",
    "            hit_count += 1\n",
    "\n",
    "    recall_at_k = hit_count / len(qa_pairs)\n",
    "    print(f\"📌 Recall@{k}: {recall_at_k:.4f}\")\n",
    "    return recall_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recall@K: 87599it [01:35, 914.73it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Recall@10: 0.6768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "recall_dpr = compute_dpr_recall(\n",
    "    qa_pairs=qa_pairs,\n",
    "    corpus_df=corpus_df,\n",
    "    ctx_emb_path=\"/mnt/aix7101/jeong/aix_project/dpr_ctx_embeddings2.npy\",\n",
    "    q_emb_path=\"/mnt/aix7101/jeong/aix_project/dpr_q_embeddings2.npy\",\n",
    "    k=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPR-m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dprm_recall(\n",
    "    qa_pairs: pd.DataFrame,\n",
    "    corpus_df: pd.DataFrame,\n",
    "    ctx_emb_path: str,\n",
    "    q_emb_path: str,\n",
    "    k: int = 5,\n",
    "    aggregation: str = \"max\",  # or \"mean\"\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    문장 단위의 문서 임베딩을 사용하여 DPR-m 방식의 Recall@k 계산.\n",
    "\n",
    "    Args:\n",
    "        qa_pairs (pd.DataFrame): 질문-정답 쌍 (columns: ['question', 'answer', 'doc_id'])\n",
    "        corpus_df (pd.DataFrame): 문서 집합 (columns: ['doc_id', 'text'])\n",
    "        ctx_emb_path (str): 문장 단위 문서 임베딩 저장 경로 (.npy, shape: [num_docs, num_sents, dim])\n",
    "        q_emb_path (str): 질문 임베딩 저장 경로 (.npy, shape: [num_queries, dim])\n",
    "        k (int): Recall@k\n",
    "        aggregation (str): 'max' 또는 'mean' 방식으로 문서 유사도 집계\n",
    "        \n",
    "    Returns:\n",
    "        float: Recall@k\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 임베딩 로드\n",
    "    ctx_embeddings = np.load(ctx_emb_path)     # shape: (num_docs, num_sents, dim)\n",
    "    q_embeddings = np.load(q_emb_path)         # shape: (num_queries, dim)\n",
    "\n",
    "    assert len(q_embeddings) == len(qa_pairs), \"❗ 질문 임베딩 수와 QA 쌍 수가 일치하지 않습니다.\"\n",
    "\n",
    "    hit_count = 0\n",
    "\n",
    "    # 2. 각 질문에 대해 문서들과 유사도 계산\n",
    "    for idx, row in tqdm(qa_pairs.iterrows(), total=len(qa_pairs), desc=\"Evaluating DPR-m Recall@K\"):\n",
    "        gt_doc_id = row[\"doc_id\"]\n",
    "        q_emb = q_embeddings[idx]                     # shape: (dim,)\n",
    "        \n",
    "        # 문서별 문장들과 유사도 → shape: (num_docs, num_sents)\n",
    "        dot_products = np.einsum(\"ijk,k->ij\", ctx_embeddings, q_emb)  # 효율적인 벡터 연산\n",
    "\n",
    "        # 문서 단위 유사도 집계\n",
    "        if aggregation == \"max\":\n",
    "            scores = np.max(dot_products, axis=1)     # (num_docs,)\n",
    "        elif aggregation == \"mean\":\n",
    "            scores = np.mean(dot_products, axis=1)\n",
    "        else:\n",
    "            raise ValueError(\"aggregation은 'max' 또는 'mean'이어야 합니다.\")\n",
    "\n",
    "        # top-k 문서 인덱스 추출\n",
    "        topk_indices = np.argsort(scores)[::-1][:k]\n",
    "        topk_doc_ids = corpus_df.iloc[topk_indices][\"doc_id\"].tolist()\n",
    "\n",
    "        # 정답 포함 여부 확인\n",
    "        if gt_doc_id in topk_doc_ids:\n",
    "            hit_count += 1\n",
    "\n",
    "    recall_at_k = hit_count / len(qa_pairs)\n",
    "    print(f\"📌 DPR-m Recall@{k} ({aggregation} aggregation): {recall_at_k:.4f}\")\n",
    "    return recall_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dprm_recall(\n",
    "    qa_pairs=qa_pairs,\n",
    "    corpus_df=corpus_df,\n",
    "    ctx_emb_path=\"/mnt/aix7101/jeong/aix_project/dpr_m_ctx_embeddings2.npy\",\n",
    "    q_emb_path=\"/mnt/aix7101/jeong/aix_project/dpr_q_embeddings2.npy\",\n",
    "    k=5,\n",
    "    aggregation=\"max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hybrid (bm25 + DPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hybrid_recall(\n",
    "    qa_pairs: pd.DataFrame,\n",
    "    corpus_df: pd.DataFrame,\n",
    "    ctx_emb_path: str,\n",
    "    q_emb_path: str,\n",
    "    bm25_top_n: int = 100,\n",
    "    k: int = 5\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    BM25 + DPR hybrid retrieval 기반 Recall@k 계산\n",
    "\n",
    "    Args:\n",
    "        qa_pairs (pd.DataFrame): 질문-정답 쌍 (columns: ['question', 'answer', 'doc_id'])\n",
    "        corpus_df (pd.DataFrame): 문서 집합 (columns: ['doc_id', 'text'])\n",
    "        ctx_emb_path (str): DPR 문서 임베딩 경로 (.npy, shape: [num_docs, dim])\n",
    "        q_emb_path (str): DPR 질문 임베딩 경로 (.npy, shape: [num_queries, dim])\n",
    "        bm25_top_n (int): BM25로 먼저 선택할 후보 문서 개수\n",
    "        k (int): 최종 DPR top-k에서 정답 포함 여부 평가\n",
    "\n",
    "    Returns:\n",
    "        float: Recall@k\n",
    "    \"\"\"\n",
    "    # 1. 임베딩 불러오기\n",
    "    ctx_embeddings = np.load(ctx_emb_path)     # shape: (num_docs, dim)\n",
    "    q_embeddings = np.load(q_emb_path)         # shape: (num_queries, dim)\n",
    "    assert len(q_embeddings) == len(qa_pairs), \"❗ 질문 임베딩 수와 QA 쌍 수가 일치하지 않습니다.\"\n",
    "\n",
    "    # 2. BM25 인덱스 구성\n",
    "    tokenized_corpus = [doc.split() for doc in corpus_df[\"text\"]]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "    hit_count = 0\n",
    "\n",
    "    # 3. 각 질문에 대해 hybrid retrieval 수행\n",
    "    for idx, row in tqdm(qa_pairs.iterrows(), total=len(qa_pairs), desc=\"Evaluating Hybrid Recall@K\"):\n",
    "        question = row[\"question\"]\n",
    "        gt_doc_id = row[\"doc_id\"]\n",
    "        q_emb = q_embeddings[idx]  # (dim,)\n",
    "\n",
    "        # (1) BM25 후보 추출\n",
    "        tokenized_query = question.split()\n",
    "        bm25_scores = bm25.get_scores(tokenized_query)\n",
    "        bm25_top_indices = np.argsort(bm25_scores)[::-1][:bm25_top_n]\n",
    "\n",
    "        # (2) DPR 유사도 계산 (bm25 후보에 한해)\n",
    "        candidate_ctx_embs = ctx_embeddings[bm25_top_indices]  # (bm25_top_n, dim)\n",
    "        dpr_scores = np.dot(candidate_ctx_embs, q_emb)         # (bm25_top_n,)\n",
    "\n",
    "        # (3) DPR 기반 top-k 문서 선택\n",
    "        topk_local_indices = np.argsort(dpr_scores)[::-1][:k]\n",
    "        topk_doc_indices = [bm25_top_indices[i] for i in topk_local_indices]\n",
    "        topk_doc_ids = corpus_df.iloc[topk_doc_indices][\"doc_id\"].tolist()\n",
    "\n",
    "        # (4) 정답 포함 여부 확인\n",
    "        if gt_doc_id in topk_doc_ids:\n",
    "            hit_count += 1\n",
    "\n",
    "    recall_at_k = hit_count / len(qa_pairs)\n",
    "    print(f\"📌 Hybrid Recall@{k} (BM25 top-{bm25_top_n} + DPR top-{k}): {recall_at_k:.4f}\")\n",
    "    return recall_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_hybrid_recall(\n",
    "    qa_pairs=qa_pairs,\n",
    "    corpus_df=corpus_df,\n",
    "    ctx_emb_path=\"/mnt/aix7101/jeong/aix_project/dpr_ctx_embeddings2.npy\",\n",
    "    q_emb_path=\"/mnt/aix7101/jeong/aix_project/dpr_q_embeddings2.npy\",\n",
    "    bm25_top_n=20,\n",
    "    k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Retrieval 구성 요소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- keyword extraction function\n",
    "# 1. rule-based \n",
    "\n",
    "# 2. keyBERT\n",
    "\n",
    "# 3. Hybrid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- custom checking code\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "j_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
