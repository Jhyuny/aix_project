{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import json\n",
    "\n",
    "def load_jsonl(filepath):\n",
    "    data = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "# Dataset loading and processing\n",
    "def load_dataset_from_huggingface():\n",
    "    dataset = load_dataset(\"nq_open\", split=\"train[:1000]\")\n",
    "    return dataset\n",
    "\n",
    "# Pickle functions\n",
    "def save_pickle(obj, filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_json(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# DPR embeddings generation\n",
    "def generate_dpr_embeddings(corpus):\n",
    "    dpr_ctx_encoder = SentenceTransformer('facebook-dpr-ctx_encoder-multiset-base')\n",
    "    embeddings = dpr_ctx_encoder.encode(corpus, batch_size=64, convert_to_tensor=False, show_progress_bar=True)\n",
    "    return embeddings\n",
    "\n",
    "def generate_query_embeddings(queries):\n",
    "    dpr_question_encoder = SentenceTransformer('facebook-dpr-question_encoder-multiset-base')\n",
    "    query_embeddings = dpr_question_encoder.encode(queries, batch_size=64, convert_to_tensor=False, show_progress_bar=True)\n",
    "    return query_embeddings\n",
    "\n",
    "# FAISS index creation for DPR\n",
    "def build_faiss_index(embeddings):\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(embeddings)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from keybert import KeyBERT\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "# Load spacy model for rule-based extraction\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "WH_WORDS = {\"what\", \"who\", \"whom\", \"where\", \"when\", \"why\", \"how\"}\n",
    "\n",
    "def extract_keyphrases_spacy(question: str):\n",
    "    doc = nlp(question.lower())\n",
    "    keyphrases = set()\n",
    "\n",
    "    wh_word = None\n",
    "    for token in doc:\n",
    "        if token.text in WH_WORDS:\n",
    "            wh_word = token.text\n",
    "            break\n",
    "\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if any(not token.is_stop and token.pos_ in {\"NOUN\", \"PROPN\"} for token in chunk):\n",
    "            keyphrases.add(chunk.text.strip())\n",
    "    if wh_word:\n",
    "        hint_map = {\n",
    "            \"who\": \"person\",\n",
    "            \"where\": \"location\",\n",
    "            \"when\": \"time\",\n",
    "            \"why\": \"reason\",\n",
    "            \"how\": \"method\",\n",
    "        }\n",
    "        hint = hint_map.get(wh_word)\n",
    "        if hint:\n",
    "            keyphrases.add(hint)\n",
    "\n",
    "    return list(keyphrases)\n",
    "\n",
    "\n",
    "\n",
    "# KeyBERT based extraction\n",
    "kw_model = KeyBERT(model='all-MiniLM-L6-v2')\n",
    "\n",
    "def extract_keyphrases_keybert(question: str, top_n: int = 5, diversity: bool = False) -> List[str]:\n",
    "    question_clean = re.sub(r\"[^\\w\\s]\", \"\", question.lower())  # Í∞ÑÎã®Ìïú Ï†ÑÏ≤òÎ¶¨\n",
    "\n",
    "    if diversity:\n",
    "        keyphrases = kw_model.extract_keywords(\n",
    "            question_clean,\n",
    "            keyphrase_ngram_range=(1, 3),\n",
    "            stop_words='english',\n",
    "            use_mmr=True,\n",
    "            diversity=0.7,\n",
    "            top_n=top_n\n",
    "        )\n",
    "    else:\n",
    "        keyphrases = kw_model.extract_keywords(\n",
    "            question_clean,\n",
    "            keyphrase_ngram_range=(1, 3),\n",
    "            stop_words='english',\n",
    "            top_n=top_n\n",
    "        )\n",
    "    return [phrase for phrase, _ in keyphrases]\n",
    "\n",
    "\n",
    "# # Unified extraction interface\n",
    "# def extract_keywords(text, method=\"spacy\", topk=5):\n",
    "#     if method == \"spacy\":\n",
    "#         return spacy_extract(text, topk)\n",
    "#     elif method == \"keybert\":\n",
    "#         return keybert_extract(text, topk)\n",
    "#     else:\n",
    "#         raise ValueError(\"Unknown extraction method\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rank_bm25 import BM25Okapi\n",
    "import torch\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "# Í∞ÑÎã®Ìïú ÌÜ†ÌÅ∞ Í∏∞Î∞ò F1 Í≥ÑÏÇ∞Í∏∞\n",
    "def compute_simple_f1(references, predictions):\n",
    "    f1_scores = []\n",
    "    for ref, pred in zip(references, predictions):\n",
    "        ref_tokens = set(ref.lower().split())\n",
    "        pred_tokens = set(pred.lower().split())\n",
    "        common = ref_tokens & pred_tokens\n",
    "        if len(common) == 0:\n",
    "            f1_scores.append(0)\n",
    "            continue\n",
    "        precision = len(common) / len(pred_tokens)\n",
    "        recall = len(common) / len(ref_tokens)\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "        f1_scores.append(f1)\n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "\n",
    "# Í∏∞Ï°¥ BM25 + generator ÌèâÍ∞ÄÏö© Ìï®Ïàò\n",
    "def bm25_retrieval_with_generation(\n",
    "    qa_pairs: pd.DataFrame,\n",
    "    corpus_df: pd.DataFrame,\n",
    "    generator_tokenizer,\n",
    "    generator_model,\n",
    "    k: int = 5,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    generator_model.to(device)\n",
    "    generator_model.eval()\n",
    "\n",
    "    tokenized_corpus = [doc.split() for doc in corpus_df[\"text\"]]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for _, row in tqdm(qa_pairs.iterrows(), total=len(qa_pairs), desc=\"BM25 + Generator Evaluation\"):\n",
    "        question = row[\"question\"]\n",
    "        gt_answer = row[\"answer\"]\n",
    "\n",
    "        tokenized_query = question.split()\n",
    "        scores = bm25.get_scores(tokenized_query)\n",
    "        topk_indices = np.argsort(scores)[::-1][:k]\n",
    "        topk_passages = corpus_df.iloc[topk_indices][\"text\"].tolist()\n",
    "\n",
    "        # generatorÏóê ÎÑ£ÏùÑ input ÏÉùÏÑ±\n",
    "        prompt = f\"question: {question} context: {' '.join(topk_passages)}\"\n",
    "        inputs = generator_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = generator_model.generate(**inputs, max_length=64)\n",
    "            answer = generator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        predictions.append(answer.strip())\n",
    "        references.append(gt_answer.strip())\n",
    "\n",
    "    acc = accuracy_score(references, predictions)\n",
    "    f1 = compute_simple_f1(references, predictions)\n",
    "\n",
    "    print(f\"üìå BM25+Generator Accuracy: {acc:.4f}\")\n",
    "    print(f\"üìå BM25+Generator F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return acc, f1\n",
    "\n",
    "\n",
    "# DPR retrieval\n",
    "def dpr_retrieval_with_generation(\n",
    "    qa_pairs, \n",
    "    corpus_df, \n",
    "    q_embeddings, \n",
    "    ctx_embeddings, \n",
    "    generator_tokenizer, \n",
    "    generator_model, \n",
    "    k=5, \n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    generator_model.to(device)\n",
    "    generator_model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for idx, row in tqdm(qa_pairs.iterrows(), total=len(qa_pairs), desc=\"DPR + Generator Evaluation\"):\n",
    "        question = row[\"question\"]\n",
    "        gt_answer = row[\"answer\"]\n",
    "        q_emb = q_embeddings[idx]\n",
    "\n",
    "        # DPR retrieval\n",
    "        scores = np.dot(ctx_embeddings, q_emb)\n",
    "        topk_indices = np.argsort(scores)[::-1][:k]\n",
    "        topk_passages = corpus_df.iloc[topk_indices][\"text\"].tolist()\n",
    "\n",
    "        # generator input ÏÉùÏÑ±\n",
    "        prompt = f\"question: {question} context: {' '.join(topk_passages)}\"\n",
    "        inputs = generator_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = generator_model.generate(**inputs, max_length=64)\n",
    "            answer = generator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        predictions.append(answer.strip())\n",
    "        references.append(gt_answer.strip())\n",
    "\n",
    "    acc = accuracy_score(references, predictions)\n",
    "    f1 = compute_simple_f1(references, predictions)\n",
    "\n",
    "    print(f\"üìå DPR+Generator Accuracy: {acc:.4f}\")\n",
    "    print(f\"üìå DPR+Generator F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return acc, f1\n",
    "\n",
    "\n",
    "# Sentence DPR retrieval\n",
    "def s_dpr_retrieval(qa_pairs, corpus_df, q_embeddings, ctx_embeddings, k=5, aggregation=\"max\"):\n",
    "\n",
    "    assert len(q_embeddings) == len(qa_pairs), \"‚ùó ÏßàÎ¨∏ ÏûÑÎ≤†Îî© ÏàòÏôÄ QA Ïåç ÏàòÍ∞Ä ÏùºÏπòÌïòÏßÄ ÏïäÏäµÎãàÎã§.\"\n",
    "\n",
    "    hit_count = 0\n",
    "    for idx, row in tqdm(qa_pairs.iterrows(), total=len(qa_pairs), desc=\"Evaluating DPR-m Recall@K\"):\n",
    "        gt_doc_id = row[\"doc_id\"]\n",
    "        q_emb = q_embeddings[idx]\n",
    "\n",
    "        scores = []\n",
    "        for doc_sents in ctx_embeddings:\n",
    "            sent_scores = np.dot(doc_sents, q_emb)\n",
    "            if aggregation == \"max\":\n",
    "                score = np.max(sent_scores)\n",
    "            elif aggregation == \"mean\":\n",
    "                score = np.mean(sent_scores)\n",
    "            else:\n",
    "                raise ValueError(\"aggregationÏùÄ 'max' ÎòêÎäî 'mean'Ïù¥Ïñ¥Ïïº Ìï©ÎãàÎã§.\")\n",
    "            scores.append(score)\n",
    "\n",
    "        scores = np.array(scores)\n",
    "        topk_indices = np.argsort(scores)[::-1][:k]\n",
    "        topk_doc_ids = corpus_df.iloc[topk_indices][\"doc_id\"].tolist()\n",
    "\n",
    "        if gt_doc_id in topk_doc_ids:\n",
    "            hit_count += 1\n",
    "    recall_at_k = hit_count / len(qa_pairs)\n",
    "    print(f\"üìå sentenceDPR Recall@{k} ({aggregation} aggregation): {recall_at_k:.4f}\")\n",
    "    return recall_at_k\n",
    "\n",
    "\n",
    "# Hybrid retrieval (BM25 + DPR)\n",
    "def hybrid_retrieval(qa_pairs, corpus_df, q_embeddings, ctx_embeddings, bm25_top_n=300, k=5):\n",
    "\n",
    "    tokenized_corpus = [doc.split() for doc in corpus_df[\"text\"]]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "    hit_count = 0\n",
    "\n",
    "    for idx, row in tqdm(qa_pairs.iterrows(), total=len(qa_pairs), desc=\"Evaluating Hybrid Recall@K\"):\n",
    "        question = row[\"question\"]\n",
    "        gt_doc_id = row[\"doc_id\"]\n",
    "        q_emb = q_embeddings[idx]  \n",
    "\n",
    "\n",
    "        tokenized_query = question.split()\n",
    "        bm25_scores = bm25.get_scores(tokenized_query)\n",
    "        bm25_top_indices = np.argsort(bm25_scores)[::-1][:bm25_top_n]\n",
    "\n",
    "        candidate_ctx_embs = ctx_embeddings[bm25_top_indices] \n",
    "        dpr_scores = np.dot(candidate_ctx_embs, q_emb)         \n",
    "\n",
    "        topk_local_indices = np.argsort(dpr_scores)[::-1][:k]\n",
    "        topk_doc_indices = [bm25_top_indices[i] for i in topk_local_indices]\n",
    "        topk_doc_ids = corpus_df.iloc[topk_doc_indices][\"doc_id\"].tolist()\n",
    "\n",
    "        if gt_doc_id in topk_doc_ids:\n",
    "            hit_count += 1\n",
    "\n",
    "    recall_at_k = hit_count / len(qa_pairs)\n",
    "    print(f\"üìå Hybrid Recall@{k} (BM25 top-{bm25_top_n} + DPR top-{k}): {recall_at_k:.4f}\")\n",
    "    return recall_at_k\n",
    "\n",
    "# Ours retrieval\n",
    "def key_retrieval_with_generation(\n",
    "    ctx_tokenizer,\n",
    "    ctx_encoder,\n",
    "    qa_pairs,\n",
    "    corpus_df,\n",
    "    q_embeddings,\n",
    "    ctx_embeddings,\n",
    "    extract_keyphrases_fn,\n",
    "    generator_tokenizer,\n",
    "    generator_model,\n",
    "    top_n_per_keyphrase=300,\n",
    "    k=5,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    generator_model.to(device)\n",
    "    generator_model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for idx, row in tqdm(qa_pairs.iterrows(), total=len(qa_pairs), desc=\"Keyphrase + Generator Evaluation\"):\n",
    "        question = row[\"question\"]\n",
    "        gt_answer = row[\"answer\"]\n",
    "\n",
    "        # 1Ô∏è‚É£ Keyphrase Ï∂îÏ∂ú\n",
    "        keyphrases = extract_keyphrases_fn(question)\n",
    "        if not keyphrases:\n",
    "            continue  # ÌÇ§ÏõåÎìú Î™ªÎΩëÏùÄ Í≤ΩÏö∞ Ïä§ÌÇµ\n",
    "\n",
    "        inputs = ctx_tokenizer(\n",
    "            keyphrases,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            phrase_embs = ctx_encoder(**inputs).pooler_output.cpu().numpy()\n",
    "\n",
    "        # 2Ô∏è‚É£ Candidate documents ÏÑ†Ï†ï\n",
    "        candidate_indices = set()\n",
    "        for emb in phrase_embs:\n",
    "            scores = np.dot(ctx_embeddings, emb)\n",
    "            top_indices = np.argsort(scores)[::-1][:top_n_per_keyphrase]\n",
    "            candidate_indices.update(top_indices)\n",
    "\n",
    "        if not candidate_indices:\n",
    "            continue\n",
    "\n",
    "        query_emb = q_embeddings[idx]\n",
    "        candidate_indices = list(candidate_indices)\n",
    "        candidate_embs = ctx_embeddings[candidate_indices]\n",
    "        rerank_scores = np.dot(candidate_embs, query_emb)\n",
    "\n",
    "        top_k_indices = np.argsort(rerank_scores)[::-1][:k]\n",
    "        top_k_passages = corpus_df.iloc[[candidate_indices[i] for i in top_k_indices]][\"text\"].tolist()\n",
    "\n",
    "        # 3Ô∏è‚É£ GeneratorÏóê ÎÑ£Í∏∞\n",
    "        prompt = f\"question: {question} context: {' '.join(top_k_passages)}\"\n",
    "        gen_inputs = generator_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = generator_model.generate(**gen_inputs, max_length=64)\n",
    "            answer = generator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        predictions.append(answer.strip())\n",
    "        references.append(gt_answer.strip())\n",
    "\n",
    "    acc = accuracy_score(references, predictions)\n",
    "    f1 = compute_simple_f1(references, predictions)\n",
    "\n",
    "    print(f\"üìå Keyphrase+Generator Accuracy: {acc:.4f}\")\n",
    "    print(f\"üìå Keyphrase+Generator F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus and queries...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Îç∞Ïù¥ÌÑ∞ Î°úÎî©\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading corpus and queries...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[43mload_jsonl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m corpus_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(corpus)\n\u001b[1;32m     32\u001b[0m queries \u001b[38;5;241m=\u001b[39m load_jsonl(query_path)\n",
      "File \u001b[0;32m~/jeong/aix_project/test_nq/dataset.py:15\u001b[0m, in \u001b[0;36mload_jsonl\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filepath, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m---> 15\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/j_project/lib/python3.9/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "\n",
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "from dataset import load_jsonl, load_json, build_faiss_index, generate_dpr_embeddings, generate_query_embeddings\n",
    "from retrieval import bm25_retrieval_with_generation, dpr_retrieval_with_generation, s_dpr_retrieval, hybrid_retrieval, key_retrieval_with_generation\n",
    "from key_extract import extract_keyphrases_spacy, extract_keyphrases_keybert\n",
    "\n",
    "\n",
    "\n",
    "generator_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "generator_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
    "dataset_dir = \"/mnt/aix7101/jeong/aix_project\"\n",
    "corpus_path = f\"{dataset_dir}/nq_rag_corpus.json\"\n",
    "query_path = f\"{dataset_dir}/nq_rag_qa_pairs.json\"\n",
    "corpus_emb_path = f\"{dataset_dir}/wiki_dpr_embeddings.npy\"\n",
    "corpus_sentence_emb_path = f\"{dataset_dir}/wiki_s_dpr_embeddings.npy\"\n",
    "query_emb_path = f\"{dataset_dir}/nq_q_embeddings.npy\"\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Î°úÎî©\n",
    "print(\"Loading corpus and queries...\")\n",
    "corpus = load_jsonl(corpus_path)\n",
    "corpus_df = pd.DataFrame(corpus)\n",
    "queries = load_jsonl(query_path)\n",
    "queries_df = pd.DataFrame(queries)\n",
    "\n",
    "# ÏûÑÎ≤†Îî© Î°úÎî©\n",
    "print(\"Loading pre-computed embeddings...\")\n",
    "corpus_embeddings = np.load(corpus_emb_path)\n",
    "query_embeddings = np.load(query_emb_path)\n",
    "\n",
    "ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\")\n",
    "ctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\")\n",
    "\n",
    "# q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")\n",
    "# q_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")\n",
    "\n",
    "ctx_encoder.eval()\n",
    "# q_encoder.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ctx_encoder.to(device)\n",
    "# q_encoder.to(device)\n",
    "\n",
    "extract_keyphrases_fn = extract_keyphrases_keybert\n",
    "\n",
    "# bm25_retrieval_with_generation(queries_df, corpus_df, generator_tokenizer, generator_model, k=5)\n",
    "# dpr_retrieval_with_generation(queries_df, corpus_df, query_embeddings, corpus_embeddings, generator_tokenizer, generator_model, k=5)\n",
    "# s_dpr_retrieval(queries_df, corpus_df, query_embeddings, corpus_embeddings, k=5)\n",
    "# hybrid_retrieval(queries_df, corpus_df, query_embeddings, corpus_embeddings, k=5)\n",
    "key_retrieval_with_generation(ctx_tokenizer, ctx_encoder, queries_df, corpus_df, query_embeddings, corpus_embeddings, extract_keyphrases_fn, generator_tokenizer, generator_model, k=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "j_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
