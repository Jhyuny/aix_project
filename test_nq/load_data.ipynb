{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'where did they film hot tub time machine', 'answer': ['Fernie Alpine Resort']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# nq_open 데이터셋 로드\n",
    "dataset = load_dataset(\"nq_open\", split=\"train\")\n",
    "\n",
    "# 예시 확인\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60472da27dc740baafc59d6b84299138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/157 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af1904ff00944f2981b8fddb1109ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/21015300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71df6a6362d04739b4bec823a9e77196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '1', 'text': 'Aaron Aaron ( or ; \"Ahärôn\") is a prophet, high priest, and the brother of Moses in the Abrahamic religions. Knowledge of Aaron, along with his brother Moses, comes exclusively from religious texts, such as the Bible and Quran. The Hebrew Bible relates that, unlike Moses, who grew up in the Egyptian royal court, Aaron and his elder sister Miriam remained with their kinsmen in the eastern border-land of Egypt (Goshen). When Moses first confronted the Egyptian king about the Israelites, Aaron served as his brother\\'s spokesman (\"prophet\") to the Pharaoh. Part of the Law (Torah) that Moses received from', 'title': 'Aaron', 'embeddings': [0.013342111371457577, 0.582173764705658, -0.31309744715690613, -0.6991612911224365, -0.5583199858665466, 0.5187504887580872, 0.7152731418609619, -0.08567414432764053, -0.24895088374614716, -0.4495537281036377, -0.643000066280365, 0.11746902763843536, -0.22123917937278748, 0.30100083351135254, 0.08902842551469803, 0.018262844532728195, 0.35032016038894653, 0.5387764573097229, -0.7160236835479736, 0.27768489718437195, -0.07402290403842926, 0.05736348405480385, -0.2623903751373291, 0.11998274177312851, 0.42047083377838135, 0.32202771306037903, 0.4316750466823578, -0.3319248855113983, -0.3958595395088196, -0.29490378499031067, 0.1877806931734085, -0.100398488342762, 0.5196011066436768, -0.20901942253112793, -0.21604545414447784, -0.17848671972751617, -0.11988848447799683, 0.08588268607854843, 0.31618618965148926, 0.31577184796333313, -0.057537250220775604, -0.5773798227310181, 0.25274738669395447, -0.03731808438897133, -0.4586816728115082, -0.2606494724750519, -0.7278411388397217, 0.8470156192779541, -0.024892883375287056, 0.12136458605527878, -0.4922543168067932, -0.05716506764292717, -0.5419178009033203, 0.6617814302444458, -0.01026879157871008, -0.08240453153848648, -0.3834538757801056, -0.0217374786734581, 0.25595659017562866, -0.32065433263778687, 0.3364619016647339, 0.2793622612953186, 0.9436274170875549, 0.07419943809509277, -0.028591636568307877, 0.36722615361213684, -0.35942503809928894, 0.08018945157527924, -0.6012433767318726, 0.18680493533611298, 0.10338036715984344, -0.24818070232868195, -0.9503740668296814, -0.037816066294908524, 0.04864095151424408, -0.38971173763275146, 0.1560625433921814, -0.016165366396307945, 0.03465748578310013, 0.35003718733787537, -0.5292726755142212, -0.12466200441122055, 0.29246458411216736, 0.19088231027126312, 0.05515093356370926, 0.33809250593185425, 0.16220323741436005, 0.16643284261226654, -0.08910343796014786, -0.8794567584991455, -0.20117390155792236, 0.23538599908351898, 0.05153215304017067, -0.028052087873220444, -0.4111497402191162, 0.2003425806760788, 0.3162536323070526, -0.184242844581604, -0.10369937121868134, -0.4574664831161499, 0.1168856993317604, 0.10409945249557495, 0.5903887748718262, -0.10240547358989716, -0.13237743079662323, -0.14929600059986115, 0.19012916088104248, -0.32198792695999146, -0.2629361152648926, 0.2668221592903137, -0.23492242395877838, 0.5787959694862366, 0.16600461304187775, -0.024004686623811722, -0.06353788077831268, 0.39425766468048096, -0.4836723506450653, -1.0324077606201172, -0.3863924443721771, -0.06947479397058487, -0.06976303458213806, 0.6292197108268738, 0.4815850555896759, 0.8333263993263245, -0.1614970713853836, -0.5836285352706909, 0.04946994036436081, 0.44299933314323425, -0.5966780781745911, 0.009543153457343578, 0.09636517614126205, 0.06079412251710892, 0.45286867022514343, -0.05853470414876938, -0.5595552921295166, -0.32627490162849426, -0.321654349565506, 0.22931836545467377, -0.015266075730323792, -0.155192032456398, -1.099725365638733, -0.4443606734275818, 0.12090086936950684, -0.11218881607055664, 0.08053071051836014, -0.24572324752807617, -0.029322126880288124, -0.25510692596435547, -0.278866708278656, 0.16946637630462646, -0.28219300508499146, 0.1638929396867752, 0.09862808138132095, -0.48473629355430603, -0.18139319121837616, 0.35590529441833496, 0.18584416806697845, 0.22782593965530396, 0.11231634020805359, 0.14086107909679413, 0.16855865716934204, 0.16393591463565826, 0.063820980489254, -0.3430526554584503, 0.16281935572624207, 0.16581737995147705, 0.33747172355651855, -0.41819822788238525, 0.0025368088390678167, -0.5437764525413513, -0.16048917174339294, -0.27151182293891907, 0.2485852986574173, -0.5246210098266602, -0.10285931825637817, 0.02890552580356598, 0.4926091730594635, -0.17282643914222717, 0.12301001697778702, -0.48179197311401367, -0.7173861265182495, 0.03650764748454094, -0.5600326657295227, -0.577528178691864, 0.09422600269317627, 0.25925469398498535, -0.2989250421524048, 0.15679173171520233, -0.13336147367954254, 0.09616518020629883, -0.5716033577919006, -0.37886980175971985, -0.1733996421098709, -0.047748956829309464, 0.3439715504646301, -0.6006388068199158, 0.015615168027579784, 0.6865774989128113, 0.2951256036758423, 0.44723591208457947, -0.5507193207740784, 0.05156802386045456, -0.011735874228179455, 0.38653671741485596, -0.4721370339393616, 0.48308005928993225, -0.5819752216339111, 0.5008648633956909, 0.545811653137207, 0.15721268951892853, 0.4570438265800476, -0.6446462869644165, 0.008594613522291183, -0.22547177970409393, 0.08848125487565994, 0.45527178049087524, 0.11119621247053146, -0.45049574971199036, 0.059182897210121155, 0.15203087031841278, 0.596472978591919, -0.2697761058807373, -0.1555441915988922, 0.24279746413230896, -0.26650312542915344, 0.14212872087955475, 0.3272002041339874, 0.12318886816501617, -0.20520636439323425, -0.24159811437129974, 0.2878652811050415, -0.16519775986671448, 0.6804537773132324, -0.14532572031021118, -0.3919063210487366, -0.18149390816688538, 0.3687795400619507, -0.17275765538215637, 0.6418197751045227, -0.027582824230194092, 0.3877091705799103, 0.2563461363315582, -0.3286064565181732, -0.38848012685775757, 0.6039966940879822, 0.1983191817998886, -0.2691623270511627, -0.24335584044456482, -0.4288327395915985, 0.793018639087677, -0.4474162459373474, -0.05150119587779045, 0.5939141511917114, -0.47305309772491455, 0.637814462184906, -0.47850024700164795, -0.030141232535243034, 0.23852652311325073, 0.8140543103218079, 0.19942307472229004, -0.0010131633607670665, 0.576574444770813, -0.5792076587677002, -0.3121621608734131, 0.003611992811784148, -0.0035351801197975874, 0.09528223425149918, 0.25566601753234863, 0.29840555787086487, 0.3033490478992462, 1.0012692213058472, -0.1723470836877823, -0.3795674741268158, 0.022254690527915955, 0.32964348793029785, -0.5092184543609619, 0.4767880439758301, 0.43636539578437805, 0.5528505444526672, -0.5531790256500244, -0.12880346179008484, 0.22618228197097778, -0.3151434361934662, -0.03973354026675224, -0.2652219831943512, -0.4793930947780609, -0.2868035137653351, -0.905595064163208, 0.10350733995437622, 0.11173075437545776, 0.394121378660202, 0.7869462370872498, 0.6168612837791443, -0.07899240404367447, 0.34136301279067993, 0.5755362510681152, 0.28675124049186707, -0.2225918173789978, 0.21988601982593536, -0.2786274552345276, -0.45123475790023804, -0.17960773408412933, -0.21113760769367218, 0.457698792219162, 0.5189108848571777, 0.4346103370189667, -0.22238492965698242, 0.6370369791984558, -5.745935440063477, -0.4969028830528259, -0.3840221166610718, 0.022046135738492012, 0.6478630304336548, -0.137510746717453, 0.6751518845558167, -0.3845255672931671, 0.6922675967216492, 0.30573010444641113, 0.34625381231307983, -0.2520020604133606, 0.2816044092178345, 0.14808008074760437, 0.09274668991565704, 0.456719309091568, 0.3370796740055084, -0.6463026404380798, 0.20837996900081635, 0.025062687695026398, 0.16107310354709625, -0.5261400938034058, -0.9588971734046936, 0.7716624140739441, 0.007130398415029049, -0.05409559607505798, -0.5882000923156738, 0.03135319799184799, -0.7391355633735657, -0.19058559834957123, -0.20751623809337616, -0.3089848756790161, -0.13430340588092804, -0.39386796951293945, -0.20612117648124695, -0.3507562279701233, 0.4656156003475189, -0.15378302335739136, 0.08091875910758972, -0.22894199192523956, -0.3663587272167206, -0.31257665157318115, -0.2496020495891571, -0.04917147010564804, 0.14395107328891754, -0.18363872170448303, -0.8159858584403992, -0.07167480140924454, -0.07025245577096939, 0.23874300718307495, -0.29845693707466125, 0.18639935553073883, 0.06178617477416992, -0.20091980695724487, 0.26853713393211365, -0.046518705785274506, 0.32080182433128357, 0.40860798954963684, -0.13383924961090088, 0.0849938839673996, 0.25926926732063293, -0.5862594842910767, -0.16909103095531464, -0.08061639219522476, 0.4513208866119385, -0.128934845328331, 0.14394256472587585, -0.5972687005996704, 0.1929595023393631, -0.12097825109958649, -0.7723067998886108, 0.2681928277015686, 0.09970526397228241, -0.9875441193580627, 0.2791946530342102, -0.18674223124980927, 0.18502698838710785, 0.4536034166812897, -0.4841155707836151, -0.5014390349388123, -0.42376574873924255, -0.62204509973526, 0.6096557378768921, 0.016337107867002487, 0.2733354866504669, 0.4336029291152954, 0.04663529619574547, -0.3526301383972168, 0.22757884860038757, 0.4601760804653168, 0.8722789883613586, -0.4028926491737366, 0.2983832359313965, 0.3099161386489868, 0.006797840818762779, 0.5444064736366272, -0.5126181244850159, 0.5441772937774658, 0.8038768172264099, 0.3460986018180847, -0.2168547362089157, 0.002366922330111265, 0.05954704433679581, -0.10748684406280518, 0.21137787401676178, 0.4554608166217804, -0.43163007497787476, 0.25133755803108215, 0.30361658334732056, 0.06174817681312561, -0.24107496440410614, 0.31094011664390564, -0.33090052008628845, -0.09494759887456894, 0.6232355237007141, 0.20254461467266083, 0.35264331102371216, 1.0044695138931274, 0.8436576128005981, 1.1823155879974365, -0.5941954851150513, 0.30281803011894226, -0.3799954652786255, -0.11119156330823898, -0.08680906891822815, -0.34019094705581665, -0.07897806912660599, -0.08845642954111099, 0.4635826647281647, -0.06214538589119911, 0.7164416909217834, 0.06339499354362488, -0.16835464537143707, 0.11125967651605606, 0.12001282721757889, -0.5300414562225342, -0.057382844388484955, -0.8737900257110596, -0.1722751259803772, -0.057661570608615875, -0.9281012415885925, -0.0411529578268528, 0.20299847424030304, 0.4360838830471039, 0.10082262009382248, -0.24507708847522736, -0.3387431800365448, 0.03348888084292412, -0.27641600370407104, 0.5392748713493347, 0.4797119200229645, 0.01551057118922472, 0.9765687584877014, 0.29106953740119934, -0.31877174973487854, 0.07055041939020157, -0.2910116910934448, 0.4656400978565216, -0.6375656127929688, 0.2486547976732254, -0.31931719183921814, -0.8066365122795105, -0.5733237862586975, -0.0494222529232502, 0.23460964858531952, 0.26430708169937134, -0.10929836332798004, -0.4255754053592682, 0.3565710186958313, 0.3791630268096924, 0.15731991827487946, -0.24865180253982544, -0.3653840124607086, -0.3432982861995697, 0.729200541973114, 0.3671967685222626, -0.13996000587940216, 0.51570725440979, -0.16123220324516296, -0.3300400376319885, -0.21838270127773285, 0.07763613760471344, -0.43250131607055664, -0.23851189017295837, 0.3580610454082489, 0.08029165863990784, -0.33338966965675354, -1.03665292263031, -0.6987199187278748, 0.02439035288989544, 0.2788645923137665, -0.06608114391565323, -0.44936057925224304, 0.546510636806488, -0.1296209990978241, -0.4272401034832001, 0.47409582138061523, -0.4952409565448761, 0.23164556920528412, -0.22899983823299408, -0.09274420142173767, 0.15485498309135437, -0.5276297926902771, 0.026999076828360558, -0.12746402621269226, -0.5999284386634827, 0.3046010434627533, 0.3349684774875641, 0.14781221747398376, -0.10549195110797882, 0.07820522040128708, -0.372130811214447, -0.593668520450592, 0.4301963448524475, 0.18750408291816711, -0.20319558680057526, 0.3408954441547394, -0.14943164587020874, 0.16451559960842133, -0.17478689551353455, 0.2591470777988434, -0.48879995942115784, 0.30594566464424133, 0.407215416431427, 0.4833449721336365, 0.4775369465351105, 0.09993761777877808, -0.22387194633483887, 0.4008619487285614, -0.3312329649925232, 0.32568955421447754, 0.6423519849777222, 0.15374714136123657, 0.7465161085128784, -0.7500524520874023, -0.15752257406711578, 0.28506720066070557, 0.483536034822464, -0.19415827095508575, 0.397411584854126, 0.3059113621711731, 0.24965520203113556, -0.039587799459695816, -0.42057734727859497, 0.669467031955719, -0.04279704764485359, -0.3522455096244812, -0.5816063284873962, -0.006955984514206648, -0.2486751675605774, 0.48806583881378174, -0.13502894341945648, -0.10889037698507309, -0.16293615102767944, 0.8560498356819153, -0.1609228402376175, -0.14554370939731598, -0.514249324798584, -0.36700940132141113, -0.33139559626579285, 0.05600138008594513, -0.40672966837882996, -0.14108705520629883, -0.3056793808937073, -0.13683877885341644, -0.20228971540927887, -0.23225994408130646, -0.3287922143936157, 0.07010006904602051, -0.4260731637477875, -0.7256983518600464, -0.8079238533973694, 0.3285459280014038, 0.4343706965446472, -0.33967113494873047, 0.4931037127971649, -0.6669278740882874, 0.1280277520418167, -0.12090429663658142, 0.1341475248336792, -0.9275414943695068, -0.2043219804763794, -0.031693123281002045, 0.2993864119052887, 0.4125460088253021, -0.2647762596607208, -0.17118121683597565, 0.10917486995458603, 0.340258926153183, 0.008467217907309532, -0.5836611390113831, 0.4534720182418823, -0.18661309778690338, 0.22191967070102692, -0.46791550517082214, -0.1673247516155243, 0.6950961351394653, 0.6230984926223755, -0.06833136081695557, -0.2921275794506073, 0.15839208662509918, 0.31407544016838074, 0.5224513411521912, 0.4880180358886719, -0.2806287705898285, 0.5517997741699219, 0.11446849256753922, -0.09426851570606232, -0.5370336174964905, 0.3783377707004547, 0.26005545258522034, -0.12884382903575897, 0.6570649743080139, -0.010207049548625946, -0.667052686214447, 0.016771147027611732, 0.7395182251930237, 0.028792696073651314, -0.17188698053359985, 0.005625546909868717, 0.16337908804416656, -0.09353276342153549, 0.21933451294898987, 0.12842266261577606, 0.009508350864052773, 0.5731686353683472, 0.513626217842102, 0.1574011594057083, -0.0928046926856041, 0.06037212163209915, 0.34846699237823486, 0.2959064245223999, -0.5177446007728577, -0.1623641550540924, 0.20998820662498474, 0.19181598722934723, 0.2055983692407608, -0.1721590906381607, 0.15389899909496307, 0.7246823906898499, 0.09028657525777817, 0.8275163173675537, -0.7378642559051514, 0.19947899878025055, -0.027157744392752647, 0.6812571883201599, 0.24400798976421356, -0.31869834661483765, 0.004607081413269043, 0.8053483366966248, -0.2750285267829895, -0.012644030153751373, 0.32843026518821716, -0.41769587993621826, 0.10478366911411285, 0.9217999577522278, 0.5367670655250549, 0.19652174413204193, -0.3339986503124237, 0.120143823325634, 0.18377183377742767, -0.07194815576076508, 0.5458688735961914, -0.1967276632785797, -0.2981669008731842, -0.07121703773736954, 0.17999732494354248, -1.0598938465118408, -0.33158645033836365, -0.11468132585287094, 0.025547195225954056, 0.020341170951724052, -0.8446840047836304, -0.23143057525157928, 0.4299076795578003, 0.1395767480134964, 0.9780437350273132, -0.06875360757112503, -0.19017060101032257, 0.5967233180999756, 0.2108098268508911, -0.3805564343929291, -0.20678094029426575, -0.9000175595283508, -0.49099963903427124, 0.5177686810493469, -0.03827724978327751, -0.2201634645462036, -1.499563217163086, 0.23444972932338715, 0.15166305005550385, -0.31187883019447327, 0.4112609624862671, 0.5527058243751526, 0.08738730847835541, 0.1013394445180893, -0.44980981945991516, 0.7611082196235657, -0.28494423627853394, 0.2644638121128082, 0.040533315390348434, 0.27798736095428467, 0.2967148721218109, -0.222095787525177, 0.03268064185976982, 0.09654885530471802, -0.38599273562431335, -1.0435175895690918, -0.7873338460922241, 0.05377493426203728, 0.36096814274787903, -0.144956573843956, 0.2724911868572235, -0.5067102909088135, -0.23126086592674255, -0.4167267084121704, 0.333368182182312, -0.5543072819709778, 0.2413502335548401, -0.1094605103135109, 0.0023073970805853605, -0.2537267804145813, -0.5412203073501587, 0.6002154350280762, -0.3583946228027344, 0.12887059152126312, 0.41921257972717285, -0.04532704874873161, -0.5824840664863586, 0.4221176207065582, 0.46539178490638733, -0.7833795547485352, -0.40958112478256226, -0.5585734248161316, -0.13919009268283844, -0.8180659413337708, 0.3868069052696228, 0.8758425116539001, 0.5391188859939575, -0.764029860496521, -0.15418075025081635, 0.09660996496677399, -0.16624252498149872, 0.16759417951107025, -0.5543164610862732, 0.5374137163162231, -0.6768856644630432, -0.5294336676597595, -0.1276760995388031, 0.8119913339614868, 0.321498304605484, 0.09892559796571732, -0.017379622906446457, 0.03613365441560745, 0.44289925694465637, -0.2320089042186737, 0.3312029540538788, -0.5385938286781311, 0.8093984127044678, -0.4741983711719513]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# DPR Wikipedia passages 로드\n",
    "corpus = load_dataset(\"wiki_dpr\", \"psgs_w100.nq.no_index\", split=\"train[:5000000]\")\n",
    "\n",
    "# 샘플 확인\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df = pd.DataFrame({\n",
    "    \"doc_id\": corpus[\"id\"],\n",
    "    \"title\": corpus[\"title\"],\n",
    "    \"text\": corpus[\"text\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df.to_json(\"/mnt/aix7101/jeong/aix_project/nq_rag_corpus.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Aaron</td>\n",
       "      <td>Aaron Aaron ( or ; \"Ahärôn\") is a prophet, hig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Aaron</td>\n",
       "      <td>God at Sinai granted Aaron the priesthood for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Aaron</td>\n",
       "      <td>his rod turn into a snake. Then he stretched o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Aaron</td>\n",
       "      <td>however, Aaron and Hur remained below to look ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Aaron</td>\n",
       "      <td>Aaron and his sons to the priesthood, and arra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc_id  title                                               text\n",
       "0      1  Aaron  Aaron Aaron ( or ; \"Ahärôn\") is a prophet, hig...\n",
       "1      2  Aaron  God at Sinai granted Aaron the priesthood for ...\n",
       "2      3  Aaron  his rod turn into a snake. Then he stretched o...\n",
       "3      4  Aaron  however, Aaron and Hur remained below to look ...\n",
       "4      5  Aaron  Aaron and his sons to the priesthood, and arra..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'where did they film hot tub time machine',\n",
       " 'answer': ['Fernie Alpine Resort']}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_data = []\n",
    "for item in dataset:\n",
    "    question = item[\"question\"]\n",
    "    answers = item[\"answer\"]\n",
    "    \n",
    "    # answers가 비어있을 수도 있어서 첫 정답만\n",
    "    answer = answers[0] if answers else \"\"\n",
    "    \n",
    "    qa_data.append({\n",
    "        \"question\": question,\n",
    "        \"answer\": answer\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = pd.DataFrame(qa_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs.to_json(\"qa_pairs.json\", orient=\"records\", lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus 예시:\n",
      "  doc_id  title                                               text\n",
      "0      1  Aaron  Aaron Aaron ( or ; \"Ahärôn\") is a prophet, hig...\n",
      "1      2  Aaron  God at Sinai granted Aaron the priesthood for ...\n",
      "2      3  Aaron  his rod turn into a snake. Then he stretched o...\n",
      "3      4  Aaron  however, Aaron and Hur remained below to look ...\n",
      "4      5  Aaron  Aaron and his sons to the priesthood, and arra...\n",
      "\n",
      "QA 쌍 예시:\n",
      "                                            question                answer\n",
      "0           where did they film hot tub time machine  Fernie Alpine Resort\n",
      "1   who has the right of way in international waters        Neither vessel\n",
      "2            who does annie work for attack on titan                Marley\n",
      "3  when was the immigration reform and control ac...      November 6, 1986\n",
      "4              when was puerto rico added to the usa                  1950\n"
     ]
    }
   ],
   "source": [
    "print(\"Corpus 예시:\")\n",
    "print(corpus_df.head())\n",
    "\n",
    "print(\"\\nQA 쌍 예시:\")\n",
    "print(qa_pairs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. Load DPR model and tokenizer (use multi-qa)\n",
    "ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "ctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "\n",
    "q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "q_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DPRQuestionEncoder(\n",
       "  (question_encoder): DPREncoder(\n",
       "    (bert_model): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx_encoder.eval()\n",
    "q_encoder.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ctx_encoder.to(device)\n",
    "q_encoder.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding contexts:   0%|          | 469/1313457 [00:19<15:30:55, 23.51it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m batch_texts \u001b[38;5;241m=\u001b[39m corpus_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      6\u001b[0m batch_texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(t)\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m batch_texts]\n\u001b[0;32m----> 8\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mctx_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/anaconda3/envs/j_project/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2883\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2881\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2882\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2883\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2885\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/anaconda3/envs/j_project/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2969\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2964\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2965\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2966\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2967\u001b[0m         )\n\u001b[1;32m   2968\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m-> 2969\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2971\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2981\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2982\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2985\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2986\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2987\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2988\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2989\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   2990\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2991\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3007\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3008\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/j_project/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3160\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3150\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3151\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3152\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3153\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3157\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3158\u001b[0m )\n\u001b[0;32m-> 3160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3162\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3178\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/j_project/lib/python3.9/site-packages/transformers/tokenization_utils.py:803\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    801\u001b[0m     ids, pair_ids \u001b[38;5;241m=\u001b[39m ids_or_pair_ids\n\u001b[0;32m--> 803\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(pair_ids) \u001b[38;5;28;01mif\u001b[39;00m pair_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    805\u001b[0m input_ids\u001b[38;5;241m.\u001b[39mappend((first_ids, second_ids))\n",
      "File \u001b[0;32m~/anaconda3/envs/j_project/lib/python3.9/site-packages/transformers/tokenization_utils.py:771\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    770\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize(text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_tokens_to_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_split_into_words:\n",
      "File \u001b[0;32m~/anaconda3/envs/j_project/lib/python3.9/site-packages/transformers/tokenization_utils.py:649\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_tokens_to_ids\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    647\u001b[0m ids \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[0;32m--> 649\u001b[0m     ids\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_token_to_id_with_added_voc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "File \u001b[0;32m~/anaconda3/envs/j_project/lib/python3.9/site-packages/transformers/tokenization_utils.py:658\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._convert_token_to_id_with_added_voc\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_encoder:\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_encoder[token]\n\u001b[0;32m--> 658\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_token_to_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/j_project/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py:169\u001b[0m, in \u001b[0;36mBertTokenizer._convert_token_to_id\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    166\u001b[0m         split_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwordpiece_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m split_tokens\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_convert_token_to_id\u001b[39m(\u001b[38;5;28mself\u001b[39m, token):\n\u001b[1;32m    170\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Converts a token (str) in an id using the vocab.\"\"\"\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mget(token, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munk_token))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "ctx_embeddings = []\n",
    "\n",
    "for i in tqdm(range(0, len(corpus_df), batch_size), desc=\"Encoding contexts\"):\n",
    "    batch_texts = corpus_df[\"text\"].iloc[i:i+batch_size].tolist()\n",
    "    batch_texts = [str(t).strip() for t in batch_texts]\n",
    "\n",
    "    inputs = ctx_tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = ctx_encoder(**inputs)\n",
    "        emb_batch = output.pooler_output.cpu().numpy()  # or output.last_hidden_state[:, 0]\n",
    "        ctx_embeddings.append(emb_batch)\n",
    "\n",
    "ctx_embeddings = np.vstack(ctx_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding questions: 100%|██████████| 87925/87925 [08:58<00:00, 163.40it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32  # 필요에 따라 조정 가능\n",
    "q_embeddings = []\n",
    "\n",
    "questions = qa_pairs[\"question\"].tolist()\n",
    "\n",
    "for i in tqdm(range(0, len(questions), batch_size), desc=\"Encoding questions\"):\n",
    "    batch_questions = questions[i:i+batch_size]\n",
    "    batch_questions = [str(q).strip() for q in batch_questions]\n",
    "\n",
    "    inputs = q_tokenizer(batch_questions, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = q_encoder(**inputs)\n",
    "        emb_batch = output.pooler_output.cpu().numpy()  # or output.last_hidden_state[:, 0]\n",
    "        q_embeddings.append(emb_batch)\n",
    "\n",
    "q_embeddings = np.vstack(q_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Question embeddings saved to: /mnt/aix7101/jeong/aix_project/nq_q_embeddings.npy\n"
     ]
    }
   ],
   "source": [
    "# 4. 저장\n",
    "embedding_dir = \"/mnt/aix7101/jeong/aix_project\"\n",
    "if not os.path.exists(embedding_dir):\n",
    "    os.makedirs(embedding_dir)\n",
    "    print(f\"📁 Created directory: {embedding_dir}\")\n",
    "\n",
    "ctx_path = os.path.join(embedding_dir, \"nq_ctx_embeddings.npy\")\n",
    "# q_path = os.path.join(embedding_dir, \"nq_q_embeddings.npy\")\n",
    "\n",
    "np.save(ctx_path, ctx_embeddings)\n",
    "# np.save(q_path, q_embeddings)\n",
    "\n",
    "print(f\"✅ Context embeddings saved to: {ctx_path}\")\n",
    "# print(f\"✅ Question embeddings saved to: {q_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 16  # GPU 상황에 따라 조정\n",
    "ctx_sentence_embeddings = []\n",
    "\n",
    "for doc in tqdm(corpus_df[\"text\"], desc=\"Encoding multi-sentence contexts\"):\n",
    "    # 1. 문서 내 문장 분리\n",
    "    sentences = sent_tokenize(doc)\n",
    "    doc_embeddings = []\n",
    "\n",
    "    # 2. 문장들을 배치로 처리\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch_sents = sentences[i:i+batch_size]\n",
    "        inputs = ctx_tokenizer(\n",
    "            batch_sents,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=128\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = ctx_encoder(**inputs)\n",
    "            emb_batch = output.pooler_output.cpu().numpy()  # or last_hidden_state[:, 0]\n",
    "            doc_embeddings.append(emb_batch)\n",
    "\n",
    "    # 3. 문서 하나에 대한 (문장 수, dim) 배열 생성\n",
    "    doc_embeddings = np.vstack(doc_embeddings)\n",
    "    ctx_sentence_embeddings.append(doc_embeddings)\n",
    "    \n",
    "# 4. 문서별 문장 수가 달라 3D 배열로 만들고 싶을 경우\n",
    "max_len = max(e.shape[0] for e in ctx_sentence_embeddings)\n",
    "dim = ctx_sentence_embeddings[0].shape[1]\n",
    "\n",
    "padded_embeddings = np.zeros((len(ctx_sentence_embeddings), max_len, dim))\n",
    "for i, emb in enumerate(ctx_sentence_embeddings):\n",
    "    padded_embeddings[i, :emb.shape[0], :] = emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 저장\n",
    "embedding_dir = \"/mnt/aix7101/jeong/aix_project\"\n",
    "if not os.path.exists(embedding_dir):\n",
    "    os.makedirs(embedding_dir)\n",
    "    print(f\"📁 Created directory: {embedding_dir}\")\n",
    "\n",
    "sentence_ctx_path = os.path.join(embedding_dir, \"nq_dpr_s_ctx_embeddings_multiqa.npy\")\n",
    "ctx_sentence_embeddings = np.array(ctx_sentence_embeddings, dtype=object)\n",
    "np.save(sentence_ctx_path, ctx_sentence_embeddings, allow_pickle=True)\n",
    "\n",
    "print(f\"✅ Context Sentence embeddings saved to: {sentence_ctx_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "j_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
