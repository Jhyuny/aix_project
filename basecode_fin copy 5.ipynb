{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus 예시:\n",
      "   doc_id                                               text\n",
      "0       0  While Japan had a large number of submarines, ...\n",
      "1       1  Other critics, such as Francis Fukuyama, note ...\n",
      "2       2  During the 1990s after NAFTA was signed, indus...\n",
      "3       3  Pre-sectarian Buddhism is the earliest phase o...\n",
      "4       4  As the Industrial Revolution spread across Eur...\n",
      "\n",
      "QA 쌍 예시:\n",
      "                                            question  \\\n",
      "0  To whom did the Virgin Mary allegedly appear i...   \n",
      "1  What is in front of the Notre Dame Main Building?   \n",
      "2  The Basilica of the Sacred heart at Notre Dame...   \n",
      "3                  What is the Grotto at Notre Dame?   \n",
      "4  What sits on top of the Main Building at Notre...   \n",
      "\n",
      "                                    answer  doc_id  \\\n",
      "0               Saint Bernadette Soubirous    7437   \n",
      "1                a copper statue of Christ    7437   \n",
      "2                        the Main Building    7437   \n",
      "3  a Marian place of prayer and reflection    7437   \n",
      "4       a golden statue of the Virgin Mary    7437   \n",
      "\n",
      "                                             context  \n",
      "0  Architecturally, the school has a Catholic cha...  \n",
      "1  Architecturally, the school has a Catholic cha...  \n",
      "2  Architecturally, the school has a Catholic cha...  \n",
      "3  Architecturally, the school has a Catholic cha...  \n",
      "4  Architecturally, the school has a Catholic cha...  \n"
     ]
    }
   ],
   "source": [
    "# 1. SQuAD 데이터 로드 (훈련 세트 기준, validation도 가능)\n",
    "dataset = load_dataset(\"squad\", split=\"train\")\n",
    "\n",
    "# 2. 고유한 context 문서 집합 생성 (retrieval corpus로 사용)\n",
    "unique_contexts = list(set(dataset[\"context\"]))\n",
    "corpus_df = pd.DataFrame({\"doc_id\": list(range(len(unique_contexts))), \"text\": unique_contexts})\n",
    "\n",
    "# 3. QA 쌍 구성 (질문, 정답, 해당 문서)\n",
    "qa_data = []\n",
    "context_to_id = {context: idx for idx, context in enumerate(unique_contexts)}\n",
    "\n",
    "for item in dataset:\n",
    "    question = item[\"question\"]\n",
    "    answer = item[\"answers\"][\"text\"][0] if item[\"answers\"][\"text\"] else \"\"\n",
    "    context = item[\"context\"]\n",
    "    doc_id = context_to_id[context]\n",
    "    qa_data.append({\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"doc_id\": doc_id,\n",
    "        \"context\": context\n",
    "    })\n",
    "\n",
    "qa_pairs = pd.DataFrame(qa_data)\n",
    "\n",
    "# 4. 결과 미리 보기\n",
    "print(\"Corpus 예시:\")\n",
    "print(corpus_df.head())\n",
    "\n",
    "print(\"\\nQA 쌍 예시:\")\n",
    "print(qa_pairs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. corpus 저장 (Retrieval 문서들)\n",
    "# corpus_records = corpus_df.to_dict(orient=\"records\")\n",
    "# with open(\"dataset/squad_rag_corpus2.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(corpus_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# # 2. QA 쌍 저장 (질문-정답-문서 매핑)\n",
    "# qa_records = qa_pairs.to_dict(orient=\"records\")\n",
    "# with open(\"dataset/squad_rag_qa_pairs2.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(qa_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# print(\"✅ JSON 파일 저장 완료:\")\n",
    "# print(\"- squad_rag_corpus.json\")\n",
    "# print(\"- squad_rag_qa_pairs.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save embedded vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-multiset-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-multiset-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. Load DPR model and tokenizer (use multi-qa)\n",
    "ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\")\n",
    "ctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\")\n",
    "\n",
    "q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")\n",
    "q_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DPRQuestionEncoder(\n",
       "  (question_encoder): DPREncoder(\n",
       "    (bert_model): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx_encoder.eval()\n",
    "q_encoder.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ctx_encoder.to(device)\n",
    "q_encoder.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding contexts: 100%|██████████| 591/591 [01:52<00:00,  5.26it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "ctx_embeddings = []\n",
    "\n",
    "for i in tqdm(range(0, len(corpus_df), batch_size), desc=\"Encoding contexts\"):\n",
    "    batch_texts = corpus_df[\"text\"].iloc[i:i+batch_size].tolist()\n",
    "    batch_texts = [str(t).strip() for t in batch_texts]\n",
    "\n",
    "    inputs = ctx_tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = ctx_encoder(**inputs)\n",
    "        emb_batch = output.pooler_output.cpu().numpy()  # or output.last_hidden_state[:, 0]\n",
    "        ctx_embeddings.append(emb_batch)\n",
    "\n",
    "ctx_embeddings = np.vstack(ctx_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding questions:   0%|          | 0/2738 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding questions: 100%|██████████| 2738/2738 [00:45<00:00, 59.99it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32  # 필요에 따라 조정 가능\n",
    "q_embeddings = []\n",
    "\n",
    "questions = qa_pairs[\"question\"].tolist()\n",
    "\n",
    "for i in tqdm(range(0, len(questions), batch_size), desc=\"Encoding questions\"):\n",
    "    batch_questions = questions[i:i+batch_size]\n",
    "    batch_questions = [str(q).strip() for q in batch_questions]\n",
    "\n",
    "    inputs = q_tokenizer(batch_questions, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = q_encoder(**inputs)\n",
    "        emb_batch = output.pooler_output.cpu().numpy()  # or output.last_hidden_state[:, 0]\n",
    "        q_embeddings.append(emb_batch)\n",
    "\n",
    "q_embeddings = np.vstack(q_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Context embeddings saved to: /mnt/aix7101/jeong/aix_project/dpr_ctx_embeddings_multiqa.npy\n",
      "✅ Question embeddings saved to: /mnt/aix7101/jeong/aix_project/dpr_q_embeddings_multiqa.npy\n"
     ]
    }
   ],
   "source": [
    "# 4. 저장\n",
    "embedding_dir = \"/mnt/aix7101/jeong/aix_project\"\n",
    "if not os.path.exists(embedding_dir):\n",
    "    os.makedirs(embedding_dir)\n",
    "    print(f\"📁 Created directory: {embedding_dir}\")\n",
    "\n",
    "ctx_path = os.path.join(embedding_dir, \"dpr_ctx_embeddings_multiqa.npy\")\n",
    "q_path = os.path.join(embedding_dir, \"dpr_q_embeddings_multiqa.npy\")\n",
    "\n",
    "np.save(ctx_path, ctx_embeddings)\n",
    "np.save(q_path, q_embeddings)\n",
    "\n",
    "print(f\"✅ Context embeddings saved to: {ctx_path}\")\n",
    "print(f\"✅ Question embeddings saved to: {q_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize import sent_tokenize\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# batch_size = 16  # GPU 상황에 따라 조정\n",
    "# ctx_sentence_embeddings = []\n",
    "\n",
    "# for doc in tqdm(corpus_df[\"text\"], desc=\"Encoding multi-sentence contexts\"):\n",
    "#     # 1. 문서 내 문장 분리\n",
    "#     sentences = sent_tokenize(doc)\n",
    "#     doc_embeddings = []\n",
    "\n",
    "#     # 2. 문장들을 배치로 처리\n",
    "#     for i in range(0, len(sentences), batch_size):\n",
    "#         batch_sents = sentences[i:i+batch_size]\n",
    "#         inputs = ctx_tokenizer(\n",
    "#             batch_sents,\n",
    "#             return_tensors=\"pt\",\n",
    "#             truncation=True,\n",
    "#             padding=True,\n",
    "#             max_length=128\n",
    "#         )\n",
    "#         inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             output = ctx_encoder(**inputs)\n",
    "#             emb_batch = output.pooler_output.cpu().numpy()  # or last_hidden_state[:, 0]\n",
    "#             doc_embeddings.append(emb_batch)\n",
    "\n",
    "#     # 3. 문서 하나에 대한 (문장 수, dim) 배열 생성\n",
    "#     doc_embeddings = np.vstack(doc_embeddings)\n",
    "#     ctx_sentence_embeddings.append(doc_embeddings)\n",
    "    \n",
    "# # 4. 문서별 문장 수가 달라 3D 배열로 만들고 싶을 경우\n",
    "# max_len = max(e.shape[0] for e in ctx_sentence_embeddings)\n",
    "# dim = ctx_sentence_embeddings[0].shape[1]\n",
    "\n",
    "# padded_embeddings = np.zeros((len(ctx_sentence_embeddings), max_len, dim))\n",
    "# for i, emb in enumerate(ctx_sentence_embeddings):\n",
    "#     padded_embeddings[i, :emb.shape[0], :] = emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 4. 저장\n",
    "# embedding_dir = \"/mnt/aix7101/jeong/aix_project\"\n",
    "# if not os.path.exists(embedding_dir):\n",
    "#     os.makedirs(embedding_dir)\n",
    "#     print(f\"📁 Created directory: {embedding_dir}\")\n",
    "\n",
    "# sentence_ctx_path = os.path.join(embedding_dir, \"dpr_m_ctx_embeddings_multiqa.npy\")\n",
    "# ctx_sentence_embeddings = np.array(ctx_sentence_embeddings, dtype=object)\n",
    "# np.save(sentence_ctx_path, ctx_sentence_embeddings, allow_pickle=True)\n",
    "\n",
    "# print(f\"✅ Context Sentence embeddings saved to: {sentence_ctx_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def compute_bm25_recall(qa_pairs: pd.DataFrame, corpus_df: pd.DataFrame, k: int = 5) -> float:\n",
    "    \"\"\"\n",
    "    BM25 기반 Recall@k 계산 함수\n",
    "    \n",
    "    Args:\n",
    "        qa_pairs (pd.DataFrame): 질문-정답 쌍이 포함된 데이터프레임 (columns: ['question', 'answer', 'doc_id'])\n",
    "        corpus_df (pd.DataFrame): 문서 집합 (columns: ['doc_id', 'text'])\n",
    "        k (int): top-k 문서 중 정답이 포함되는지 평가할 k 값\n",
    "        \n",
    "    Returns:\n",
    "        float: Recall@k\n",
    "    \"\"\"\n",
    "    # 1. 토크나이즈된 문서 리스트 생성\n",
    "    tokenized_corpus = [doc.split() for doc in corpus_df[\"text\"]]\n",
    "    \n",
    "    # 2. BM25 인덱스 구성\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    \n",
    "    hit_count = 0\n",
    "\n",
    "    # 3. 각 질문에 대해 BM25 top-k 문서 검색\n",
    "    for _, row in tqdm(qa_pairs.iterrows(), total=len(qa_pairs), desc=\"Evaluating BM25 Recall@K\"):\n",
    "        question = row[\"question\"]\n",
    "        gt_doc_id = row[\"doc_id\"]\n",
    "\n",
    "        tokenized_query = question.split()\n",
    "        scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "        # 상위 k개의 문서 인덱스 추출\n",
    "        topk_indices = np.argsort(scores)[::-1][:k]\n",
    "        topk_doc_ids = corpus_df.iloc[topk_indices][\"doc_id\"].tolist()\n",
    "\n",
    "        if gt_doc_id in topk_doc_ids:\n",
    "            hit_count += 1\n",
    "\n",
    "    recall_at_k = hit_count / len(qa_pairs)\n",
    "    print(f\"📌 BM25 Recall@{k}: {recall_at_k:.4f}\")\n",
    "    return recall_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BM25 Recall@K:   0%|          | 123/87599 [00:05<1:02:03, 23.49it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m recall_bm25 \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_bm25_recall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqa_pairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 32\u001b[0m, in \u001b[0;36mcompute_bm25_recall\u001b[0;34m(qa_pairs, corpus_df, k)\u001b[0m\n\u001b[1;32m     29\u001b[0m gt_doc_id \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     31\u001b[0m tokenized_query \u001b[38;5;241m=\u001b[39m question\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m---> 32\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mbm25\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# 상위 k개의 문서 인덱스 추출\u001b[39;00m\n\u001b[1;32m     35\u001b[0m topk_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(scores)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][:k]\n",
      "File \u001b[0;32m~/anaconda3/envs/j_project/lib/python3.9/site-packages/rank_bm25.py:118\u001b[0m, in \u001b[0;36mBM25Okapi.get_scores\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    116\u001b[0m doc_len \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc_len)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m query:\n\u001b[0;32m--> 118\u001b[0m     q_freq \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdoc_freqs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midf\u001b[38;5;241m.\u001b[39mget(q) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m (q_freq \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk1 \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\n\u001b[1;32m    120\u001b[0m                                        (q_freq \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk1 \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m*\u001b[39m doc_len \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgdl)))\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m score\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "recall_bm25 = compute_bm25_recall(qa_pairs, corpus_df, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dpr_recall(qa_pairs, corpus_df, ctx_emb_path, q_emb_path, k=5):\n",
    "    \"\"\"\n",
    "    저장된 임베딩 파일을 기반으로 top-k 문서 중 정답 문서가 포함되는 비율(Recall@k)을 계산합니다.\n",
    "    \n",
    "    Args:\n",
    "        qa_pairs (pd.DataFrame): 질문-정답 쌍이 포함된 데이터프레임 (columns: ['question', 'answer', 'doc_id'])\n",
    "        corpus_df (pd.DataFrame): 문서 집합 (columns: ['doc_id', 'text'])\n",
    "        ctx_emb_path (str): 문서 임베딩이 저장된 .npy 경로\n",
    "        q_emb_path (str): 질문 임베딩이 저장된 .npy 경로\n",
    "        k (int): top-k 문서 중 정답이 포함되는지 평가할 k 값\n",
    "        \n",
    "    Returns:\n",
    "        float: Recall@k\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 임베딩 로드\n",
    "    ctx_embeddings = np.load(ctx_emb_path)\n",
    "    q_embeddings = np.load(q_emb_path)\n",
    "\n",
    "    assert len(q_embeddings) == len(qa_pairs), \"❗ 질문 임베딩 수와 QA 쌍 수가 일치하지 않습니다.\"\n",
    "\n",
    "    hit_count = 0\n",
    "\n",
    "    # 2. 각 질문에 대해 유사한 top-k 문서 검색\n",
    "    for idx, row in tqdm(qa_pairs.iterrows(), total=len(qa_pairs), desc=\"Evaluating Recall@K\"):\n",
    "        gt_doc_id = row[\"doc_id\"]\n",
    "        q_emb = q_embeddings[idx]\n",
    "\n",
    "        # 문서들과의 유사도 (cosine 유사도 대신 dot-product 사용)\n",
    "        scores = np.dot(ctx_embeddings, q_emb)\n",
    "\n",
    "        # top-k 인덱스\n",
    "        topk_indices = np.argsort(scores)[::-1][:k]\n",
    "        topk_doc_ids = corpus_df.iloc[topk_indices][\"doc_id\"].tolist()\n",
    "        # print(topk_indices)\n",
    "        # print(gt_doc_id)\n",
    "        # 정답 문서가 top-k에 포함되는지 확인\n",
    "        if gt_doc_id in topk_doc_ids:\n",
    "            hit_count += 1\n",
    "\n",
    "    recall_at_k = hit_count / len(qa_pairs)\n",
    "    print(f\"📌 Recall@{k}: {recall_at_k:.4f}\")\n",
    "    return recall_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recall@K: 100%|██████████| 87599/87599 [01:01<00:00, 1427.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Recall@3: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "recall_dpr = compute_dpr_recall(\n",
    "    qa_pairs=qa_pairs,\n",
    "    corpus_df=corpus_df,\n",
    "    ctx_emb_path=\"/mnt/aix7101/jeong/aix_project/dpr_ctx_embeddings_multiqa.npy\",\n",
    "    q_emb_path=\"/mnt/aix7101/jeong/aix_project/dpr_q_embeddings_multiqa.npy\",\n",
    "    k=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPR-m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dprm_recall(\n",
    "    qa_pairs: pd.DataFrame,\n",
    "    corpus_df: pd.DataFrame,\n",
    "    ctx_emb_path: str,\n",
    "    q_emb_path: str,\n",
    "    k: int = 5,\n",
    "    aggregation: str = \"mean\",\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    문장 단위의 문서 임베딩을 사용하여 DPR-m 방식의 Recall@k 계산.\n",
    "\n",
    "    Args:\n",
    "        qa_pairs (pd.DataFrame): 질문-정답 쌍 (columns: ['question', 'answer', 'doc_id'])\n",
    "        corpus_df (pd.DataFrame): 문서 집합 (columns: ['doc_id', 'text'])\n",
    "        ctx_emb_path (str): 문장 단위 문서 임베딩 저장 경로 (.npy, shape: [num_docs, num_sents, dim])\n",
    "        q_emb_path (str): 질문 임베딩 저장 경로 (.npy, shape: [num_queries, dim])\n",
    "        k (int): Recall@k\n",
    "        aggregation (str): 'max' 또는 'mean' 방식으로 문서 유사도 집계\n",
    "        \n",
    "    Returns:\n",
    "        float: Recall@k\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 임베딩 로드\n",
    "    ctx_embeddings = np.load(ctx_emb_path, allow_pickle=True)  # object 배열\n",
    "    q_embeddings = np.load(q_emb_path)\n",
    "\n",
    "    assert len(q_embeddings) == len(qa_pairs), \"❗ 질문 임베딩 수와 QA 쌍 수가 일치하지 않습니다.\"\n",
    "\n",
    "    hit_count = 0\n",
    "\n",
    "    for idx, row in tqdm(qa_pairs.iterrows(), total=len(qa_pairs), desc=\"Evaluating DPR-m Recall@K\"):\n",
    "        gt_doc_id = row[\"doc_id\"]\n",
    "        q_emb = q_embeddings[idx]  # (dim,)\n",
    "\n",
    "        # 각 문서에 대해 문장 임베딩과 q_emb의 유사도 계산\n",
    "        scores = []\n",
    "        for doc_sents in ctx_embeddings:\n",
    "            sent_scores = np.dot(doc_sents, q_emb)  # (num_sents,)\n",
    "            if aggregation == \"max\": # 유사도가 제일 높은 문장이 있는 것으로 할지\n",
    "                score = np.max(sent_scores)\n",
    "            elif aggregation == \"mean\": # 전체적인 문장의 평균으로 계산할지\n",
    "                score = np.mean(sent_scores)\n",
    "            else:\n",
    "                raise ValueError(\"aggregation은 'max' 또는 'mean'이어야 합니다.\")\n",
    "            scores.append(score)\n",
    "\n",
    "        scores = np.array(scores)\n",
    "        topk_indices = np.argsort(scores)[::-1][:k]\n",
    "        topk_doc_ids = corpus_df.iloc[topk_indices][\"doc_id\"].tolist()\n",
    "\n",
    "        if gt_doc_id in topk_doc_ids:\n",
    "            hit_count += 1\n",
    "\n",
    "    recall_at_k = hit_count / len(qa_pairs)\n",
    "    print(f\"📌 DPR-m Recall@{k} ({aggregation} aggregation): {recall_at_k:.4f}\")\n",
    "    return recall_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DPR-m Recall@K: 100%|██████████| 87599/87599 [1:55:38<00:00, 12.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 DPR-m Recall@3 (max aggregation): 0.6796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6795739677393577"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_dprm_recall(\n",
    "    qa_pairs=qa_pairs,\n",
    "    corpus_df=corpus_df,\n",
    "    ctx_emb_path=\"/mnt/aix7101/jeong/aix_project/dpr_m_ctx_embeddings_multiqa.npy\",\n",
    "    q_emb_path=\"/mnt/aix7101/jeong/aix_project/dpr_q_embeddings_multiqa.npy\",\n",
    "    k=3,\n",
    "    aggregation=\"max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hybrid (bm25 + DPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hybrid_recall(\n",
    "    qa_pairs: pd.DataFrame,\n",
    "    corpus_df: pd.DataFrame,\n",
    "    ctx_emb_path: str,\n",
    "    q_emb_path: str,\n",
    "    bm25_top_n: int = 100,\n",
    "    k: int = 5\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    BM25 + DPR hybrid retrieval 기반 Recall@k 계산\n",
    "\n",
    "    Args:\n",
    "        qa_pairs (pd.DataFrame): 질문-정답 쌍 (columns: ['question', 'answer', 'doc_id'])\n",
    "        corpus_df (pd.DataFrame): 문서 집합 (columns: ['doc_id', 'text'])\n",
    "        ctx_emb_path (str): DPR 문서 임베딩 경로 (.npy, shape: [num_docs, dim])\n",
    "        q_emb_path (str): DPR 질문 임베딩 경로 (.npy, shape: [num_queries, dim])\n",
    "        bm25_top_n (int): BM25로 먼저 선택할 후보 문서 개수\n",
    "        k (int): 최종 DPR top-k에서 정답 포함 여부 평가\n",
    "\n",
    "    Returns:\n",
    "        float: Recall@k\n",
    "    \"\"\"\n",
    "    # 1. 임베딩 불러오기\n",
    "    ctx_embeddings = np.load(ctx_emb_path)     # shape: (num_docs, dim)\n",
    "    q_embeddings = np.load(q_emb_path)         # shape: (num_queries, dim)\n",
    "    assert len(q_embeddings) == len(qa_pairs), \"❗ 질문 임베딩 수와 QA 쌍 수가 일치하지 않습니다.\"\n",
    "\n",
    "    # 2. BM25 인덱스 구성\n",
    "    tokenized_corpus = [doc.split() for doc in corpus_df[\"text\"]]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "    hit_count = 0\n",
    "\n",
    "    # 3. 각 질문에 대해 hybrid retrieval 수행\n",
    "    for idx, row in tqdm(qa_pairs.iterrows(), total=len(qa_pairs), desc=\"Evaluating Hybrid Recall@K\"):\n",
    "        question = row[\"question\"]\n",
    "        gt_doc_id = row[\"doc_id\"]\n",
    "        q_emb = q_embeddings[idx]  # (dim,)\n",
    "\n",
    "        # (1) BM25 후보 추출\n",
    "        tokenized_query = question.split()\n",
    "        bm25_scores = bm25.get_scores(tokenized_query)\n",
    "        bm25_top_indices = np.argsort(bm25_scores)[::-1][:bm25_top_n]\n",
    "\n",
    "        # (2) DPR 유사도 계산 (bm25 후보에 한해)\n",
    "        candidate_ctx_embs = ctx_embeddings[bm25_top_indices]  # (bm25_top_n, dim)\n",
    "        dpr_scores = np.dot(candidate_ctx_embs, q_emb)         # (bm25_top_n,)\n",
    "\n",
    "        # (3) DPR 기반 top-k 문서 선택\n",
    "        topk_local_indices = np.argsort(dpr_scores)[::-1][:k]\n",
    "        topk_doc_indices = [bm25_top_indices[i] for i in topk_local_indices]\n",
    "        topk_doc_ids = corpus_df.iloc[topk_doc_indices][\"doc_id\"].tolist()\n",
    "\n",
    "        # (4) 정답 포함 여부 확인\n",
    "        if gt_doc_id in topk_doc_ids:\n",
    "            hit_count += 1\n",
    "\n",
    "    recall_at_k = hit_count / len(qa_pairs)\n",
    "    print(f\"📌 Hybrid Recall@{k} (BM25 top-{bm25_top_n} + DPR top-{k}): {recall_at_k:.4f}\")\n",
    "    return recall_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Hybrid Recall@K:   5%|▌         | 4609/87599 [02:37<52:21, 26.41it/s]  "
     ]
    }
   ],
   "source": [
    "compute_hybrid_recall(\n",
    "    qa_pairs=qa_pairs,\n",
    "    corpus_df=corpus_df,\n",
    "    ctx_emb_path=\"/mnt/aix7101/jeong/aix_project/dpr_ctx_embeddings_multiqa.npy\",\n",
    "    q_emb_path=\"/mnt/aix7101/jeong/aix_project/dpr_q_embeddings_multiqa.npy\",\n",
    "    bm25_top_n=300,\n",
    "    k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Retrieval 구성 요소\n",
    "1. 문장에서 keyword 추출 (phrase 단위로 추출할 수 있는 방법이 있는지)\n",
    "2. 추출한 keyword와의 score도 함께 계산\n",
    "4. query만으로 추출한 recall@k\n",
    "5. keyword만으로 추출한 recall@k\n",
    "6. 둘을 hybrid하는 것도 ㄱㅊ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keyword extract function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# 1. spaCy 영어 모델 로드\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 2. 의문사 리스트 정의\n",
    "WH_WORDS = {\"what\", \"who\", \"whom\", \"where\", \"when\", \"why\", \"how\"}\n",
    "\n",
    "# 3. keyphrase 추출 함수 정의\n",
    "def extract_keyphrases_spacy(question: str):\n",
    "    doc = nlp(question.lower())\n",
    "    keyphrases = set()\n",
    "\n",
    "    wh_word = None\n",
    "    for token in doc:\n",
    "        if token.text in WH_WORDS:\n",
    "            wh_word = token.text\n",
    "            break\n",
    "\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if any(not token.is_stop and token.pos_ in {\"NOUN\", \"PROPN\"} for token in chunk):\n",
    "            keyphrases.add(chunk.text.strip())\n",
    "\n",
    "    # 의문사에 따른 힌트 키워드 추가\n",
    "    if wh_word:\n",
    "        hint_map = {\n",
    "            \"who\": \"person\",\n",
    "            \"where\": \"location\",\n",
    "            \"when\": \"time\",\n",
    "            \"why\": \"reason\",\n",
    "            \"how\": \"method\",\n",
    "        }\n",
    "        hint = hint_map.get(wh_word)\n",
    "        if hint:\n",
    "            keyphrases.add(hint)\n",
    "\n",
    "    return list(keyphrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "# KeyBERT 모델 초기화 (기본적으로 'all-MiniLM-L6-v2' 사용)\n",
    "kw_model = KeyBERT(model='all-MiniLM-L6-v2')\n",
    "\n",
    "def extract_keyphrases_keybert(question: str, top_n: int = 5, diversity: bool = False) -> List[str]:\n",
    "    \"\"\"\n",
    "    KeyBERT 기반 keyphrase 추출 함수 (의문사 힌트 없음)\n",
    "\n",
    "    Args:\n",
    "        question (str): 입력 질문\n",
    "        top_n (int): 추출할 키프레이즈 개수\n",
    "        diversity (bool): MMR(Minimal Marginal Relevance) 사용 여부\n",
    "\n",
    "    Returns:\n",
    "        List[str]: 추출된 키프레이즈 리스트\n",
    "    \"\"\"\n",
    "    question_clean = re.sub(r\"[^\\w\\s]\", \"\", question.lower())  # 간단한 전처리\n",
    "\n",
    "    if diversity:\n",
    "        keyphrases = kw_model.extract_keywords(\n",
    "            question_clean,\n",
    "            keyphrase_ngram_range=(1, 3),\n",
    "            stop_words='english',\n",
    "            use_mmr=True,\n",
    "            diversity=0.7,\n",
    "            top_n=top_n\n",
    "        )\n",
    "    else:\n",
    "        keyphrases = kw_model.extract_keywords(\n",
    "            question_clean,\n",
    "            keyphrase_ngram_range=(1, 3),\n",
    "            stop_words='english',\n",
    "            top_n=top_n\n",
    "        )\n",
    "\n",
    "    return [phrase for phrase, _ in keyphrases]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. keyphrase-based pre-filtering\n",
    "- keyphrase를 추출\n",
    "- 각 keyphrase를 embedding하고 corpus와의 유사도 계산을 통해 후보 100개씩 추출\n",
    "2. query-to-context matching\n",
    "- 전체 corpus가 아닌 후보 corpus와만 비교해서 최종 recall@k를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import normalize\n",
    "import torch\n",
    "\n",
    "def compute_dpr_hybrid_keyphrase_recall(\n",
    "    qa_pairs,\n",
    "    corpus_df,\n",
    "    ctx_emb_path,\n",
    "    extract_keyphrases_fn,\n",
    "    top_n_per_keyphrase=50,\n",
    "    final_top_k=5,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    DPR 인코더를 사용한 키프레이즈 기반 Hybrid Retrieval Recall@K 계산\n",
    "\n",
    "    Args:\n",
    "        qa_pairs (pd.DataFrame): ['question', 'doc_id']\n",
    "        corpus_df (pd.DataFrame): ['doc_id', 'text']\n",
    "        ctx_embeddings (np.ndarray): 문서 임베딩 (shape: [num_docs, dim])\n",
    "        ctx_tokenizer, ctx_encoder: DPR context 인코더\n",
    "        q_tokenizer, q_encoder: DPR query 인코더\n",
    "        extract_keyphrases_fn (function): 키프레이즈 추출 함수\n",
    "        top_n_per_keyphrase (int): 키프레이즈 당 후보 문서 수\n",
    "        final_top_k (int): 최종 선택할 문서 수\n",
    "        device (str): 'cuda' or 'cpu'\n",
    "        \n",
    "    Returns:\n",
    "        float: Recall@k\n",
    "    \"\"\"\n",
    "    hit_count = 0\n",
    "    ctx_embeddings = np.load(ctx_emb_path, allow_pickle=True)  \n",
    "    ctx_embeddings = normalize(ctx_embeddings)\n",
    "\n",
    "\n",
    "    for idx, row in tqdm(qa_pairs.iterrows(), total=len(qa_pairs), desc=\"Custom Retrieval Recall@K\"):\n",
    "        question = row[\"question\"]\n",
    "        gt_doc_id = row[\"doc_id\"]\n",
    "\n",
    "        # 1. 키프레이즈 추출\n",
    "        keyphrases = extract_keyphrases_fn(question)\n",
    "        if not keyphrases:\n",
    "            continue\n",
    "\n",
    "        # 2. 키프레이즈 임베딩\n",
    "        phrase_embs = []\n",
    "        for phrase in keyphrases:\n",
    "            inputs = ctx_tokenizer(phrase, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                emb = ctx_encoder(**inputs).pooler_output[0].cpu().numpy()\n",
    "            phrase_embs.append(emb)\n",
    "        phrase_embs = normalize(np.stack(phrase_embs))\n",
    "\n",
    "        # 3. 키워드 별 상위 문서 수집\n",
    "        candidate_indices = set()\n",
    "        for emb in phrase_embs:\n",
    "            scores = np.dot(ctx_embeddings, emb)\n",
    "            top_indices = np.argsort(scores)[::-1][:top_n_per_keyphrase]\n",
    "            candidate_indices.update(top_indices)\n",
    "\n",
    "        if not candidate_indices:\n",
    "            continue\n",
    "\n",
    "        # 4. 쿼리 임베딩\n",
    "        q_inputs = q_tokenizer(question, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
    "        q_inputs = {k: v.to(device) for k, v in q_inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            query_emb = q_encoder(**q_inputs).pooler_output[0].cpu().numpy()\n",
    "        query_emb = normalize(query_emb.reshape(1, -1))[0]\n",
    "\n",
    "        # 5. 후보 문서 재랭킹\n",
    "        candidate_indices = list(candidate_indices)\n",
    "        candidate_embs = ctx_embeddings[candidate_indices]\n",
    "        rerank_scores = np.dot(candidate_embs, query_emb)\n",
    "\n",
    "        top_k_indices = np.argsort(rerank_scores)[::-1][:final_top_k]\n",
    "        top_k_doc_ids = corpus_df.iloc[[candidate_indices[i] for i in top_k_indices]][\"doc_id\"].tolist()\n",
    "\n",
    "        if gt_doc_id in top_k_doc_ids:\n",
    "            hit_count += 1\n",
    "\n",
    "    recall_at_k = hit_count / len(qa_pairs)\n",
    "    print(f\"📌 Custom Retrieval Keyphrase-based Recall@{final_top_k}: {recall_at_k:.4f}\")\n",
    "    return recall_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Custom Retrieval Recall@K: 100%|██████████| 87599/87599 [1:15:37<00:00, 19.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Custom Retrieval Keyphrase-based Recall@5: 0.4982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "recall_spacy = compute_dpr_hybrid_keyphrase_recall(\n",
    "    qa_pairs=qa_pairs,\n",
    "    corpus_df=corpus_df,\n",
    "    ctx_emb_path=\"/mnt/aix7101/jeong/aix_project/dpr_ctx_embeddings_multiqa.npy\",\n",
    "    extract_keyphrases_fn=extract_keyphrases_spacy,  # 앞서 정의한 spaCy 기반 함수\n",
    "    top_n_per_keyphrase=100,\n",
    "    final_top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# recall 값 저장\n",
    "with open(\"recall_spacy_result.txt\", \"w\") as f:\n",
    "    f.write(f\"Recall: {recall_spacy}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Custom Retrieval Recall@K:  67%|██████▋   | 59100/87599 [1:43:19<1:15:10,  6.32it/s]"
     ]
    }
   ],
   "source": [
    "recall_keybert = compute_dpr_hybrid_keyphrase_recall(\n",
    "    qa_pairs=qa_pairs,\n",
    "    corpus_df=corpus_df,\n",
    "    ctx_emb_path=\"/mnt/aix7101/jeong/aix_project/dpr_ctx_embeddings_multiqa.npy\",\n",
    "    extract_keyphrases_fn=extract_keyphrases_keybert,  # keybert 기반 extraction\n",
    "    top_n_per_keyphrase=100,\n",
    "    final_top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall 값 저장\n",
    "with open(\"recall_keybert_result.txt\", \"w\") as f:\n",
    "    f.write(f\"Recall: {recall_keybert}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall 값 저장\n",
    "with open(\"recall_result_test.txt\", \"w\") as f:\n",
    "    f.write(f\"Recall: {recall_dpr}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- custom checking code\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "j_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
