{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus ì˜ˆì‹œ:\n",
      "   doc_id                                               text\n",
      "0       0  While Japan had a large number of submarines, ...\n",
      "1       1  Other critics, such as Francis Fukuyama, note ...\n",
      "2       2  During the 1990s after NAFTA was signed, indus...\n",
      "3       3  Pre-sectarian Buddhism is the earliest phase o...\n",
      "4       4  As the Industrial Revolution spread across Eur...\n",
      "\n",
      "QA ìŒ ì˜ˆì‹œ:\n",
      "                                            question  \\\n",
      "0  To whom did the Virgin Mary allegedly appear i...   \n",
      "1  What is in front of the Notre Dame Main Building?   \n",
      "2  The Basilica of the Sacred heart at Notre Dame...   \n",
      "3                  What is the Grotto at Notre Dame?   \n",
      "4  What sits on top of the Main Building at Notre...   \n",
      "\n",
      "                                    answer  doc_id  \\\n",
      "0               Saint Bernadette Soubirous    7437   \n",
      "1                a copper statue of Christ    7437   \n",
      "2                        the Main Building    7437   \n",
      "3  a Marian place of prayer and reflection    7437   \n",
      "4       a golden statue of the Virgin Mary    7437   \n",
      "\n",
      "                                             context  \n",
      "0  Architecturally, the school has a Catholic cha...  \n",
      "1  Architecturally, the school has a Catholic cha...  \n",
      "2  Architecturally, the school has a Catholic cha...  \n",
      "3  Architecturally, the school has a Catholic cha...  \n",
      "4  Architecturally, the school has a Catholic cha...  \n"
     ]
    }
   ],
   "source": [
    "# 1. SQuAD ë°ì´í„° ë¡œë“œ (í›ˆë ¨ ì„¸íŠ¸ ê¸°ì¤€, validationë„ ê°€ëŠ¥)\n",
    "dataset = load_dataset(\"squad\", split=\"train\")\n",
    "\n",
    "# 2. ê³ ìœ í•œ context ë¬¸ì„œ ì§‘í•© ìƒì„± (retrieval corpusë¡œ ì‚¬ìš©)\n",
    "unique_contexts = list(set(dataset[\"context\"]))\n",
    "corpus_df = pd.DataFrame({\"doc_id\": list(range(len(unique_contexts))), \"text\": unique_contexts})\n",
    "\n",
    "# 3. QA ìŒ êµ¬ì„± (ì§ˆë¬¸, ì •ë‹µ, í•´ë‹¹ ë¬¸ì„œ)\n",
    "qa_data = []\n",
    "context_to_id = {context: idx for idx, context in enumerate(unique_contexts)}\n",
    "\n",
    "for item in dataset:\n",
    "    question = item[\"question\"]\n",
    "    answer = item[\"answers\"][\"text\"][0] if item[\"answers\"][\"text\"] else \"\"\n",
    "    context = item[\"context\"]\n",
    "    doc_id = context_to_id[context]\n",
    "    qa_data.append({\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"doc_id\": doc_id,\n",
    "        \"context\": context\n",
    "    })\n",
    "\n",
    "qa_pairs = pd.DataFrame(qa_data)\n",
    "\n",
    "# 4. ê²°ê³¼ ë¯¸ë¦¬ ë³´ê¸°\n",
    "print(\"Corpus ì˜ˆì‹œ:\")\n",
    "print(corpus_df.head())\n",
    "\n",
    "print(\"\\nQA ìŒ ì˜ˆì‹œ:\")\n",
    "print(qa_pairs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. corpus ì €ì¥ (Retrieval ë¬¸ì„œë“¤)\n",
    "# corpus_records = corpus_df.to_dict(orient=\"records\")\n",
    "# with open(\"dataset/squad_rag_corpus2.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(corpus_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# # 2. QA ìŒ ì €ì¥ (ì§ˆë¬¸-ì •ë‹µ-ë¬¸ì„œ ë§¤í•‘)\n",
    "# qa_records = qa_pairs.to_dict(orient=\"records\")\n",
    "# with open(\"dataset/squad_rag_qa_pairs2.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(qa_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# print(\"âœ… JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ:\")\n",
    "# print(\"- squad_rag_corpus.json\")\n",
    "# print(\"- squad_rag_qa_pairs.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save embedded vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-multiset-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-multiset-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. Load DPR model and tokenizer (use multi-qa)\n",
    "ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\")\n",
    "ctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\")\n",
    "\n",
    "q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")\n",
    "q_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DPRQuestionEncoder(\n",
       "  (question_encoder): DPREncoder(\n",
       "    (bert_model): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx_encoder.eval()\n",
    "q_encoder.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ctx_encoder.to(device)\n",
    "q_encoder.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding contexts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 591/591 [01:52<00:00,  5.26it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "ctx_embeddings = []\n",
    "\n",
    "for i in tqdm(range(0, len(corpus_df), batch_size), desc=\"Encoding contexts\"):\n",
    "    batch_texts = corpus_df[\"text\"].iloc[i:i+batch_size].tolist()\n",
    "    batch_texts = [str(t).strip() for t in batch_texts]\n",
    "\n",
    "    inputs = ctx_tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = ctx_encoder(**inputs)\n",
    "        emb_batch = output.pooler_output.cpu().numpy()  # or output.last_hidden_state[:, 0]\n",
    "        ctx_embeddings.append(emb_batch)\n",
    "\n",
    "ctx_embeddings = np.vstack(ctx_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding questions:   0%|          | 0/2738 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding questions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2738/2738 [00:45<00:00, 59.99it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32  # í•„ìš”ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥\n",
    "q_embeddings = []\n",
    "\n",
    "questions = qa_pairs[\"question\"].tolist()\n",
    "\n",
    "for i in tqdm(range(0, len(questions), batch_size), desc=\"Encoding questions\"):\n",
    "    batch_questions = questions[i:i+batch_size]\n",
    "    batch_questions = [str(q).strip() for q in batch_questions]\n",
    "\n",
    "    inputs = q_tokenizer(batch_questions, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = q_encoder(**inputs)\n",
    "        emb_batch = output.pooler_output.cpu().numpy()  # or output.last_hidden_state[:, 0]\n",
    "        q_embeddings.append(emb_batch)\n",
    "\n",
    "q_embeddings = np.vstack(q_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Context embeddings saved to: /mnt/aix7101/jeong/aix_project/dpr_ctx_embeddings_multiqa.npy\n",
      "âœ… Question embeddings saved to: /mnt/aix7101/jeong/aix_project/dpr_q_embeddings_multiqa.npy\n"
     ]
    }
   ],
   "source": [
    "# 4. ì €ì¥\n",
    "embedding_dir = \"/mnt/aix7101/jeong/aix_project\"\n",
    "if not os.path.exists(embedding_dir):\n",
    "    os.makedirs(embedding_dir)\n",
    "    print(f\"ğŸ“ Created directory: {embedding_dir}\")\n",
    "\n",
    "ctx_path = os.path.join(embedding_dir, \"dpr_ctx_embeddings_multiqa.npy\")\n",
    "q_path = os.path.join(embedding_dir, \"dpr_q_embeddings_multiqa.npy\")\n",
    "\n",
    "np.save(ctx_path, ctx_embeddings)\n",
    "np.save(q_path, q_embeddings)\n",
    "\n",
    "print(f\"âœ… Context embeddings saved to: {ctx_path}\")\n",
    "print(f\"âœ… Question embeddings saved to: {q_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize import sent_tokenize\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# batch_size = 16  # GPU ìƒí™©ì— ë”°ë¼ ì¡°ì •\n",
    "# ctx_sentence_embeddings = []\n",
    "\n",
    "# for doc in tqdm(corpus_df[\"text\"], desc=\"Encoding multi-sentence contexts\"):\n",
    "#     # 1. ë¬¸ì„œ ë‚´ ë¬¸ì¥ ë¶„ë¦¬\n",
    "#     sentences = sent_tokenize(doc)\n",
    "#     doc_embeddings = []\n",
    "\n",
    "#     # 2. ë¬¸ì¥ë“¤ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬\n",
    "#     for i in range(0, len(sentences), batch_size):\n",
    "#         batch_sents = sentences[i:i+batch_size]\n",
    "#         inputs = ctx_tokenizer(\n",
    "#             batch_sents,\n",
    "#             return_tensors=\"pt\",\n",
    "#             truncation=True,\n",
    "#             padding=True,\n",
    "#             max_length=128\n",
    "#         )\n",
    "#         inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             output = ctx_encoder(**inputs)\n",
    "#             emb_batch = output.pooler_output.cpu().numpy()  # or last_hidden_state[:, 0]\n",
    "#             doc_embeddings.append(emb_batch)\n",
    "\n",
    "#     # 3. ë¬¸ì„œ í•˜ë‚˜ì— ëŒ€í•œ (ë¬¸ì¥ ìˆ˜, dim) ë°°ì—´ ìƒì„±\n",
    "#     doc_embeddings = np.vstack(doc_embeddings)\n",
    "#     ctx_sentence_embeddings.append(doc_embeddings)\n",
    "    \n",
    "# # 4. ë¬¸ì„œë³„ ë¬¸ì¥ ìˆ˜ê°€ ë‹¬ë¼ 3D ë°°ì—´ë¡œ ë§Œë“¤ê³  ì‹¶ì„ ê²½ìš°\n",
    "# max_len = max(e.shape[0] for e in ctx_sentence_embeddings)\n",
    "# dim = ctx_sentence_embeddings[0].shape[1]\n",
    "\n",
    "# padded_embeddings = np.zeros((len(ctx_sentence_embeddings), max_len, dim))\n",
    "# for i, emb in enumerate(ctx_sentence_embeddings):\n",
    "#     padded_embeddings[i, :emb.shape[0], :] = emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 4. ì €ì¥\n",
    "# embedding_dir = \"/mnt/aix7101/jeong/aix_project\"\n",
    "# if not os.path.exists(embedding_dir):\n",
    "#     os.makedirs(embedding_dir)\n",
    "#     print(f\"ğŸ“ Created directory: {embedding_dir}\")\n",
    "\n",
    "# sentence_ctx_path = os.path.join(embedding_dir, \"dpr_m_ctx_embeddings_multiqa.npy\")\n",
    "# ctx_sentence_embeddings = np.array(ctx_sentence_embeddings, dtype=object)\n",
    "# np.save(sentence_ctx_path, ctx_sentence_embeddings, allow_pickle=True)\n",
    "\n",
    "# print(f\"âœ… Context Sentence embeddings saved to: {sentence_ctx_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def compute_bm25_recall(qa_pairs: pd.DataFrame, corpus_df: pd.DataFrame, k: int = 5) -> float:\n",
    "    \"\"\"\n",
    "    BM25 ê¸°ë°˜ Recall@k ê³„ì‚° í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        qa_pairs (pd.DataFrame): ì§ˆë¬¸-ì •ë‹µ ìŒì´ í¬í•¨ëœ ë°ì´í„°í”„ë ˆì„ (columns: ['question', 'answer', 'doc_id'])\n",
    "        corpus_df (pd.DataFrame): ë¬¸ì„œ ì§‘í•© (columns: ['doc_id', 'text'])\n",
    "        k (int): top-k ë¬¸ì„œ ì¤‘ ì •ë‹µì´ í¬í•¨ë˜ëŠ”ì§€ í‰ê°€í•  k ê°’\n",
    "        \n",
    "    Returns:\n",
    "        float: Recall@k\n",
    "    \"\"\"\n",
    "    # 1. í† í¬ë‚˜ì´ì¦ˆëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "    tokenized_corpus = [doc.split() for doc in corpus_df[\"text\"]]\n",
    "    \n",
    "    # 2. BM25 ì¸ë±ìŠ¤ êµ¬ì„±\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    \n",
    "    hit_count = 0\n",
    "\n",
    "    # 3. ê° ì§ˆë¬¸ì— ëŒ€í•´ BM25 top-k ë¬¸ì„œ ê²€ìƒ‰\n",
    "    for _, row in tqdm(qa_pairs.iterrows(), total=len(qa_pairs), desc=\"Evaluating BM25 Recall@K\"):\n",
    "        question = row[\"question\"]\n",
    "        gt_doc_id = row[\"doc_id\"]\n",
    "\n",
    "        tokenized_query = question.split()\n",
    "        scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "        # ìƒìœ„ kê°œì˜ ë¬¸ì„œ ì¸ë±ìŠ¤ ì¶”ì¶œ\n",
    "        topk_indices = np.argsort(scores)[::-1][:k]\n",
    "        topk_doc_ids = corpus_df.iloc[topk_indices][\"doc_id\"].tolist()\n",
    "\n",
    "        if gt_doc_id in topk_doc_ids:\n",
    "            hit_count += 1\n",
    "\n",
    "    recall_at_k = hit_count / len(qa_pairs)\n",
    "    print(f\"ğŸ“Œ BM25 Recall@{k}: {recall_at_k:.4f}\")\n",
    "    return recall_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BM25 Recall@K:   0%|          | 123/87599 [00:05<1:02:03, 23.49it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m recall_bm25 \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_bm25_recall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqa_pairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 32\u001b[0m, in \u001b[0;36mcompute_bm25_recall\u001b[0;34m(qa_pairs, corpus_df, k)\u001b[0m\n\u001b[1;32m     29\u001b[0m gt_doc_id \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     31\u001b[0m tokenized_query \u001b[38;5;241m=\u001b[39m question\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m---> 32\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mbm25\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# ìƒìœ„ kê°œì˜ ë¬¸ì„œ ì¸ë±ìŠ¤ ì¶”ì¶œ\u001b[39;00m\n\u001b[1;32m     35\u001b[0m topk_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(scores)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][:k]\n",
      "File \u001b[0;32m~/anaconda3/envs/j_project/lib/python3.9/site-packages/rank_bm25.py:118\u001b[0m, in \u001b[0;36mBM25Okapi.get_scores\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    116\u001b[0m doc_len \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc_len)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m query:\n\u001b[0;32m--> 118\u001b[0m     q_freq \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdoc_freqs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midf\u001b[38;5;241m.\u001b[39mget(q) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m (q_freq \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk1 \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\n\u001b[1;32m    120\u001b[0m                                        (q_freq \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk1 \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m*\u001b[39m doc_len \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgdl)))\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m score\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "recall_bm25 = compute_bm25_recall(qa_pairs, corpus_df, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dpr_recall(qa_pairs, corpus_df, ctx_emb_path, q_emb_path, k=5):\n",
    "    \"\"\"\n",
    "    ì €ì¥ëœ ì„ë² ë”© íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ top-k ë¬¸ì„œ ì¤‘ ì •ë‹µ ë¬¸ì„œê°€ í¬í•¨ë˜ëŠ” ë¹„ìœ¨(Recall@k)ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        qa_pairs (pd.DataFrame): ì§ˆë¬¸-ì •ë‹µ ìŒì´ í¬í•¨ëœ ë°ì´í„°í”„ë ˆì„ (columns: ['question', 'answer', 'doc_id'])\n",
    "        corpus_df (pd.DataFrame): ë¬¸ì„œ ì§‘í•© (columns: ['doc_id', 'text'])\n",
    "        ctx_emb_path (str): ë¬¸ì„œ ì„ë² ë”©ì´ ì €ì¥ëœ .npy ê²½ë¡œ\n",
    "        q_emb_path (str): ì§ˆë¬¸ ì„ë² ë”©ì´ ì €ì¥ëœ .npy ê²½ë¡œ\n",
    "        k (int): top-k ë¬¸ì„œ ì¤‘ ì •ë‹µì´ í¬í•¨ë˜ëŠ”ì§€ í‰ê°€í•  k ê°’\n",
    "        \n",
    "    Returns:\n",
    "        float: Recall@k\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. ì„ë² ë”© ë¡œë“œ\n",
    "    ctx_embeddings = np.load(ctx_emb_path)\n",
    "    q_embeddings = np.load(q_emb_path)\n",
    "\n",
    "    assert len(q_embeddings) == len(qa_pairs), \"â— ì§ˆë¬¸ ì„ë² ë”© ìˆ˜ì™€ QA ìŒ ìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "    hit_count = 0\n",
    "\n",
    "    # 2. ê° ì§ˆë¬¸ì— ëŒ€í•´ ìœ ì‚¬í•œ top-k ë¬¸ì„œ ê²€ìƒ‰\n",
    "    for idx, row in tqdm(qa_pairs.iterrows(), total=len(qa_pairs), desc=\"Evaluating Recall@K\"):\n",
    "        gt_doc_id = row[\"doc_id\"]\n",
    "        q_emb = q_embeddings[idx]\n",
    "\n",
    "        # ë¬¸ì„œë“¤ê³¼ì˜ ìœ ì‚¬ë„ (cosine ìœ ì‚¬ë„ ëŒ€ì‹  dot-product ì‚¬ìš©)\n",
    "        scores = np.dot(ctx_embeddings, q_emb)\n",
    "\n",
    "        # top-k ì¸ë±ìŠ¤\n",
    "        topk_indices = np.argsort(scores)[::-1][:k]\n",
    "        topk_doc_ids = corpus_df.iloc[topk_indices][\"doc_id\"].tolist()\n",
    "        # print(topk_indices)\n",
    "        # print(gt_doc_id)\n",
    "        # ì •ë‹µ ë¬¸ì„œê°€ top-kì— í¬í•¨ë˜ëŠ”ì§€ í™•ì¸\n",
    "        if gt_doc_id in topk_doc_ids:\n",
    "            hit_count += 1\n",
    "\n",
    "    recall_at_k = hit_count / len(qa_pairs)\n",
    "    print(f\"ğŸ“Œ Recall@{k}: {recall_at_k:.4f}\")\n",
    "    return recall_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Recall@K: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87599/87599 [01:01<00:00, 1427.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ Recall@3: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "recall_dpr = compute_dpr_recall(\n",
    "    qa_pairs=qa_pairs,\n",
    "    corpus_df=corpus_df,\n",
    "    ctx_emb_path=\"/mnt/aix7101/jeong/aix_project/dpr_ctx_embeddings_multiqa.npy\",\n",
    "    q_emb_path=\"/mnt/aix7101/jeong/aix_project/dpr_q_embeddings_multiqa.npy\",\n",
    "    k=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPR-m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dprm_recall(\n",
    "    qa_pairs: pd.DataFrame,\n",
    "    corpus_df: pd.DataFrame,\n",
    "    ctx_emb_path: str,\n",
    "    q_emb_path: str,\n",
    "    k: int = 5,\n",
    "    aggregation: str = \"mean\",\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    ë¬¸ì¥ ë‹¨ìœ„ì˜ ë¬¸ì„œ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ DPR-m ë°©ì‹ì˜ Recall@k ê³„ì‚°.\n",
    "\n",
    "    Args:\n",
    "        qa_pairs (pd.DataFrame): ì§ˆë¬¸-ì •ë‹µ ìŒ (columns: ['question', 'answer', 'doc_id'])\n",
    "        corpus_df (pd.DataFrame): ë¬¸ì„œ ì§‘í•© (columns: ['doc_id', 'text'])\n",
    "        ctx_emb_path (str): ë¬¸ì¥ ë‹¨ìœ„ ë¬¸ì„œ ì„ë² ë”© ì €ì¥ ê²½ë¡œ (.npy, shape: [num_docs, num_sents, dim])\n",
    "        q_emb_path (str): ì§ˆë¬¸ ì„ë² ë”© ì €ì¥ ê²½ë¡œ (.npy, shape: [num_queries, dim])\n",
    "        k (int): Recall@k\n",
    "        aggregation (str): 'max' ë˜ëŠ” 'mean' ë°©ì‹ìœ¼ë¡œ ë¬¸ì„œ ìœ ì‚¬ë„ ì§‘ê³„\n",
    "        \n",
    "    Returns:\n",
    "        float: Recall@k\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. ì„ë² ë”© ë¡œë“œ\n",
    "    ctx_embeddings = np.load(ctx_emb_path, allow_pickle=True)  # object ë°°ì—´\n",
    "    q_embeddings = np.load(q_emb_path)\n",
    "\n",
    "    assert len(q_embeddings) == len(qa_pairs), \"â— ì§ˆë¬¸ ì„ë² ë”© ìˆ˜ì™€ QA ìŒ ìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "    hit_count = 0\n",
    "\n",
    "    for idx, row in tqdm(qa_pairs.iterrows(), total=len(qa_pairs), desc=\"Evaluating DPR-m Recall@K\"):\n",
    "        gt_doc_id = row[\"doc_id\"]\n",
    "        q_emb = q_embeddings[idx]  # (dim,)\n",
    "\n",
    "        # ê° ë¬¸ì„œì— ëŒ€í•´ ë¬¸ì¥ ì„ë² ë”©ê³¼ q_embì˜ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "        scores = []\n",
    "        for doc_sents in ctx_embeddings:\n",
    "            sent_scores = np.dot(doc_sents, q_emb)  # (num_sents,)\n",
    "            if aggregation == \"max\": # ìœ ì‚¬ë„ê°€ ì œì¼ ë†’ì€ ë¬¸ì¥ì´ ìˆëŠ” ê²ƒìœ¼ë¡œ í• ì§€\n",
    "                score = np.max(sent_scores)\n",
    "            elif aggregation == \"mean\": # ì „ì²´ì ì¸ ë¬¸ì¥ì˜ í‰ê· ìœ¼ë¡œ ê³„ì‚°í• ì§€\n",
    "                score = np.mean(sent_scores)\n",
    "            else:\n",
    "                raise ValueError(\"aggregationì€ 'max' ë˜ëŠ” 'mean'ì´ì–´ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "            scores.append(score)\n",
    "\n",
    "        scores = np.array(scores)\n",
    "        topk_indices = np.argsort(scores)[::-1][:k]\n",
    "        topk_doc_ids = corpus_df.iloc[topk_indices][\"doc_id\"].tolist()\n",
    "\n",
    "        if gt_doc_id in topk_doc_ids:\n",
    "            hit_count += 1\n",
    "\n",
    "    recall_at_k = hit_count / len(qa_pairs)\n",
    "    print(f\"ğŸ“Œ DPR-m Recall@{k} ({aggregation} aggregation): {recall_at_k:.4f}\")\n",
    "    return recall_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DPR-m Recall@K: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87599/87599 [1:55:38<00:00, 12.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ DPR-m Recall@3 (max aggregation): 0.6796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6795739677393577"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_dprm_recall(\n",
    "    qa_pairs=qa_pairs,\n",
    "    corpus_df=corpus_df,\n",
    "    ctx_emb_path=\"/mnt/aix7101/jeong/aix_project/dpr_m_ctx_embeddings_multiqa.npy\",\n",
    "    q_emb_path=\"/mnt/aix7101/jeong/aix_project/dpr_q_embeddings_multiqa.npy\",\n",
    "    k=3,\n",
    "    aggregation=\"max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hybrid (bm25 + DPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hybrid_recall(\n",
    "    qa_pairs: pd.DataFrame,\n",
    "    corpus_df: pd.DataFrame,\n",
    "    ctx_emb_path: str,\n",
    "    q_emb_path: str,\n",
    "    bm25_top_n: int = 100,\n",
    "    k: int = 5\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    BM25 + DPR hybrid retrieval ê¸°ë°˜ Recall@k ê³„ì‚°\n",
    "\n",
    "    Args:\n",
    "        qa_pairs (pd.DataFrame): ì§ˆë¬¸-ì •ë‹µ ìŒ (columns: ['question', 'answer', 'doc_id'])\n",
    "        corpus_df (pd.DataFrame): ë¬¸ì„œ ì§‘í•© (columns: ['doc_id', 'text'])\n",
    "        ctx_emb_path (str): DPR ë¬¸ì„œ ì„ë² ë”© ê²½ë¡œ (.npy, shape: [num_docs, dim])\n",
    "        q_emb_path (str): DPR ì§ˆë¬¸ ì„ë² ë”© ê²½ë¡œ (.npy, shape: [num_queries, dim])\n",
    "        bm25_top_n (int): BM25ë¡œ ë¨¼ì € ì„ íƒí•  í›„ë³´ ë¬¸ì„œ ê°œìˆ˜\n",
    "        k (int): ìµœì¢… DPR top-kì—ì„œ ì •ë‹µ í¬í•¨ ì—¬ë¶€ í‰ê°€\n",
    "\n",
    "    Returns:\n",
    "        float: Recall@k\n",
    "    \"\"\"\n",
    "    # 1. ì„ë² ë”© ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    ctx_embeddings = np.load(ctx_emb_path)     # shape: (num_docs, dim)\n",
    "    q_embeddings = np.load(q_emb_path)         # shape: (num_queries, dim)\n",
    "    assert len(q_embeddings) == len(qa_pairs), \"â— ì§ˆë¬¸ ì„ë² ë”© ìˆ˜ì™€ QA ìŒ ìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "    # 2. BM25 ì¸ë±ìŠ¤ êµ¬ì„±\n",
    "    tokenized_corpus = [doc.split() for doc in corpus_df[\"text\"]]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "    hit_count = 0\n",
    "\n",
    "    # 3. ê° ì§ˆë¬¸ì— ëŒ€í•´ hybrid retrieval ìˆ˜í–‰\n",
    "    for idx, row in tqdm(qa_pairs.iterrows(), total=len(qa_pairs), desc=\"Evaluating Hybrid Recall@K\"):\n",
    "        question = row[\"question\"]\n",
    "        gt_doc_id = row[\"doc_id\"]\n",
    "        q_emb = q_embeddings[idx]  # (dim,)\n",
    "\n",
    "        # (1) BM25 í›„ë³´ ì¶”ì¶œ\n",
    "        tokenized_query = question.split()\n",
    "        bm25_scores = bm25.get_scores(tokenized_query)\n",
    "        bm25_top_indices = np.argsort(bm25_scores)[::-1][:bm25_top_n]\n",
    "\n",
    "        # (2) DPR ìœ ì‚¬ë„ ê³„ì‚° (bm25 í›„ë³´ì— í•œí•´)\n",
    "        candidate_ctx_embs = ctx_embeddings[bm25_top_indices]  # (bm25_top_n, dim)\n",
    "        dpr_scores = np.dot(candidate_ctx_embs, q_emb)         # (bm25_top_n,)\n",
    "\n",
    "        # (3) DPR ê¸°ë°˜ top-k ë¬¸ì„œ ì„ íƒ\n",
    "        topk_local_indices = np.argsort(dpr_scores)[::-1][:k]\n",
    "        topk_doc_indices = [bm25_top_indices[i] for i in topk_local_indices]\n",
    "        topk_doc_ids = corpus_df.iloc[topk_doc_indices][\"doc_id\"].tolist()\n",
    "\n",
    "        # (4) ì •ë‹µ í¬í•¨ ì—¬ë¶€ í™•ì¸\n",
    "        if gt_doc_id in topk_doc_ids:\n",
    "            hit_count += 1\n",
    "\n",
    "    recall_at_k = hit_count / len(qa_pairs)\n",
    "    print(f\"ğŸ“Œ Hybrid Recall@{k} (BM25 top-{bm25_top_n} + DPR top-{k}): {recall_at_k:.4f}\")\n",
    "    return recall_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Hybrid Recall@K:   5%|â–Œ         | 4609/87599 [02:37<52:21, 26.41it/s]  "
     ]
    }
   ],
   "source": [
    "compute_hybrid_recall(\n",
    "    qa_pairs=qa_pairs,\n",
    "    corpus_df=corpus_df,\n",
    "    ctx_emb_path=\"/mnt/aix7101/jeong/aix_project/dpr_ctx_embeddings_multiqa.npy\",\n",
    "    q_emb_path=\"/mnt/aix7101/jeong/aix_project/dpr_q_embeddings_multiqa.npy\",\n",
    "    bm25_top_n=300,\n",
    "    k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Retrieval êµ¬ì„± ìš”ì†Œ\n",
    "1. ë¬¸ì¥ì—ì„œ keyword ì¶”ì¶œ (phrase ë‹¨ìœ„ë¡œ ì¶”ì¶œí•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ìˆëŠ”ì§€)\n",
    "2. ì¶”ì¶œí•œ keywordì™€ì˜ scoreë„ í•¨ê»˜ ê³„ì‚°\n",
    "4. queryë§Œìœ¼ë¡œ ì¶”ì¶œí•œ recall@k\n",
    "5. keywordë§Œìœ¼ë¡œ ì¶”ì¶œí•œ recall@k\n",
    "6. ë‘˜ì„ hybridí•˜ëŠ” ê²ƒë„ ã„±ã…Š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keyword extract function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# 1. spaCy ì˜ì–´ ëª¨ë¸ ë¡œë“œ\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 2. ì˜ë¬¸ì‚¬ ë¦¬ìŠ¤íŠ¸ ì •ì˜\n",
    "WH_WORDS = {\"what\", \"who\", \"whom\", \"where\", \"when\", \"why\", \"how\"}\n",
    "\n",
    "# 3. keyphrase ì¶”ì¶œ í•¨ìˆ˜ ì •ì˜\n",
    "def extract_keyphrases_spacy(question: str):\n",
    "    doc = nlp(question.lower())\n",
    "    keyphrases = set()\n",
    "\n",
    "    wh_word = None\n",
    "    for token in doc:\n",
    "        if token.text in WH_WORDS:\n",
    "            wh_word = token.text\n",
    "            break\n",
    "\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if any(not token.is_stop and token.pos_ in {\"NOUN\", \"PROPN\"} for token in chunk):\n",
    "            keyphrases.add(chunk.text.strip())\n",
    "\n",
    "    # ì˜ë¬¸ì‚¬ì— ë”°ë¥¸ íŒíŠ¸ í‚¤ì›Œë“œ ì¶”ê°€\n",
    "    if wh_word:\n",
    "        hint_map = {\n",
    "            \"who\": \"person\",\n",
    "            \"where\": \"location\",\n",
    "            \"when\": \"time\",\n",
    "            \"why\": \"reason\",\n",
    "            \"how\": \"method\",\n",
    "        }\n",
    "        hint = hint_map.get(wh_word)\n",
    "        if hint:\n",
    "            keyphrases.add(hint)\n",
    "\n",
    "    return list(keyphrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "# KeyBERT ëª¨ë¸ ì´ˆê¸°í™” (ê¸°ë³¸ì ìœ¼ë¡œ 'all-MiniLM-L6-v2' ì‚¬ìš©)\n",
    "kw_model = KeyBERT(model='all-MiniLM-L6-v2')\n",
    "\n",
    "def extract_keyphrases_keybert(question: str, top_n: int = 5, diversity: bool = False) -> List[str]:\n",
    "    \"\"\"\n",
    "    KeyBERT ê¸°ë°˜ keyphrase ì¶”ì¶œ í•¨ìˆ˜ (ì˜ë¬¸ì‚¬ íŒíŠ¸ ì—†ìŒ)\n",
    "\n",
    "    Args:\n",
    "        question (str): ì…ë ¥ ì§ˆë¬¸\n",
    "        top_n (int): ì¶”ì¶œí•  í‚¤í”„ë ˆì´ì¦ˆ ê°œìˆ˜\n",
    "        diversity (bool): MMR(Minimal Marginal Relevance) ì‚¬ìš© ì—¬ë¶€\n",
    "\n",
    "    Returns:\n",
    "        List[str]: ì¶”ì¶œëœ í‚¤í”„ë ˆì´ì¦ˆ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    question_clean = re.sub(r\"[^\\w\\s]\", \"\", question.lower())  # ê°„ë‹¨í•œ ì „ì²˜ë¦¬\n",
    "\n",
    "    if diversity:\n",
    "        keyphrases = kw_model.extract_keywords(\n",
    "            question_clean,\n",
    "            keyphrase_ngram_range=(1, 3),\n",
    "            stop_words='english',\n",
    "            use_mmr=True,\n",
    "            diversity=0.7,\n",
    "            top_n=top_n\n",
    "        )\n",
    "    else:\n",
    "        keyphrases = kw_model.extract_keywords(\n",
    "            question_clean,\n",
    "            keyphrase_ngram_range=(1, 3),\n",
    "            stop_words='english',\n",
    "            top_n=top_n\n",
    "        )\n",
    "\n",
    "    return [phrase for phrase, _ in keyphrases]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. keyphrase-based pre-filtering\n",
    "- keyphraseë¥¼ ì¶”ì¶œ\n",
    "- ê° keyphraseë¥¼ embeddingí•˜ê³  corpusì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°ì„ í†µí•´ í›„ë³´ 100ê°œì”© ì¶”ì¶œ\n",
    "2. query-to-context matching\n",
    "- ì „ì²´ corpusê°€ ì•„ë‹Œ í›„ë³´ corpusì™€ë§Œ ë¹„êµí•´ì„œ ìµœì¢… recall@kë¥¼ ê³„ì‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import normalize\n",
    "import torch\n",
    "\n",
    "def compute_dpr_hybrid_keyphrase_recall(\n",
    "    qa_pairs,\n",
    "    corpus_df,\n",
    "    ctx_emb_path,\n",
    "    extract_keyphrases_fn,\n",
    "    top_n_per_keyphrase=50,\n",
    "    final_top_k=5,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    DPR ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•œ í‚¤í”„ë ˆì´ì¦ˆ ê¸°ë°˜ Hybrid Retrieval Recall@K ê³„ì‚°\n",
    "\n",
    "    Args:\n",
    "        qa_pairs (pd.DataFrame): ['question', 'doc_id']\n",
    "        corpus_df (pd.DataFrame): ['doc_id', 'text']\n",
    "        ctx_embeddings (np.ndarray): ë¬¸ì„œ ì„ë² ë”© (shape: [num_docs, dim])\n",
    "        ctx_tokenizer, ctx_encoder: DPR context ì¸ì½”ë”\n",
    "        q_tokenizer, q_encoder: DPR query ì¸ì½”ë”\n",
    "        extract_keyphrases_fn (function): í‚¤í”„ë ˆì´ì¦ˆ ì¶”ì¶œ í•¨ìˆ˜\n",
    "        top_n_per_keyphrase (int): í‚¤í”„ë ˆì´ì¦ˆ ë‹¹ í›„ë³´ ë¬¸ì„œ ìˆ˜\n",
    "        final_top_k (int): ìµœì¢… ì„ íƒí•  ë¬¸ì„œ ìˆ˜\n",
    "        device (str): 'cuda' or 'cpu'\n",
    "        \n",
    "    Returns:\n",
    "        float: Recall@k\n",
    "    \"\"\"\n",
    "    hit_count = 0\n",
    "    ctx_embeddings = np.load(ctx_emb_path, allow_pickle=True)  \n",
    "    ctx_embeddings = normalize(ctx_embeddings)\n",
    "\n",
    "\n",
    "    for idx, row in tqdm(qa_pairs.iterrows(), total=len(qa_pairs), desc=\"Custom Retrieval Recall@K\"):\n",
    "        question = row[\"question\"]\n",
    "        gt_doc_id = row[\"doc_id\"]\n",
    "\n",
    "        # 1. í‚¤í”„ë ˆì´ì¦ˆ ì¶”ì¶œ\n",
    "        keyphrases = extract_keyphrases_fn(question)\n",
    "        if not keyphrases:\n",
    "            continue\n",
    "\n",
    "        # 2. í‚¤í”„ë ˆì´ì¦ˆ ì„ë² ë”©\n",
    "        phrase_embs = []\n",
    "        for phrase in keyphrases:\n",
    "            inputs = ctx_tokenizer(phrase, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                emb = ctx_encoder(**inputs).pooler_output[0].cpu().numpy()\n",
    "            phrase_embs.append(emb)\n",
    "        phrase_embs = normalize(np.stack(phrase_embs))\n",
    "\n",
    "        # 3. í‚¤ì›Œë“œ ë³„ ìƒìœ„ ë¬¸ì„œ ìˆ˜ì§‘\n",
    "        candidate_indices = set()\n",
    "        for emb in phrase_embs:\n",
    "            scores = np.dot(ctx_embeddings, emb)\n",
    "            top_indices = np.argsort(scores)[::-1][:top_n_per_keyphrase]\n",
    "            candidate_indices.update(top_indices)\n",
    "\n",
    "        if not candidate_indices:\n",
    "            continue\n",
    "\n",
    "        # 4. ì¿¼ë¦¬ ì„ë² ë”©\n",
    "        q_inputs = q_tokenizer(question, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
    "        q_inputs = {k: v.to(device) for k, v in q_inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            query_emb = q_encoder(**q_inputs).pooler_output[0].cpu().numpy()\n",
    "        query_emb = normalize(query_emb.reshape(1, -1))[0]\n",
    "\n",
    "        # 5. í›„ë³´ ë¬¸ì„œ ì¬ë­í‚¹\n",
    "        candidate_indices = list(candidate_indices)\n",
    "        candidate_embs = ctx_embeddings[candidate_indices]\n",
    "        rerank_scores = np.dot(candidate_embs, query_emb)\n",
    "\n",
    "        top_k_indices = np.argsort(rerank_scores)[::-1][:final_top_k]\n",
    "        top_k_doc_ids = corpus_df.iloc[[candidate_indices[i] for i in top_k_indices]][\"doc_id\"].tolist()\n",
    "\n",
    "        if gt_doc_id in top_k_doc_ids:\n",
    "            hit_count += 1\n",
    "\n",
    "    recall_at_k = hit_count / len(qa_pairs)\n",
    "    print(f\"ğŸ“Œ Custom Retrieval Keyphrase-based Recall@{final_top_k}: {recall_at_k:.4f}\")\n",
    "    return recall_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Custom Retrieval Recall@K: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87599/87599 [1:15:37<00:00, 19.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ Custom Retrieval Keyphrase-based Recall@5: 0.4982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "recall_spacy = compute_dpr_hybrid_keyphrase_recall(\n",
    "    qa_pairs=qa_pairs,\n",
    "    corpus_df=corpus_df,\n",
    "    ctx_emb_path=\"/mnt/aix7101/jeong/aix_project/dpr_ctx_embeddings_multiqa.npy\",\n",
    "    extract_keyphrases_fn=extract_keyphrases_spacy,  # ì•ì„œ ì •ì˜í•œ spaCy ê¸°ë°˜ í•¨ìˆ˜\n",
    "    top_n_per_keyphrase=100,\n",
    "    final_top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# recall ê°’ ì €ì¥\n",
    "with open(\"recall_spacy_result.txt\", \"w\") as f:\n",
    "    f.write(f\"Recall: {recall_spacy}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Custom Retrieval Recall@K:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 59100/87599 [1:43:19<1:15:10,  6.32it/s]"
     ]
    }
   ],
   "source": [
    "recall_keybert = compute_dpr_hybrid_keyphrase_recall(\n",
    "    qa_pairs=qa_pairs,\n",
    "    corpus_df=corpus_df,\n",
    "    ctx_emb_path=\"/mnt/aix7101/jeong/aix_project/dpr_ctx_embeddings_multiqa.npy\",\n",
    "    extract_keyphrases_fn=extract_keyphrases_keybert,  # keybert ê¸°ë°˜ extraction\n",
    "    top_n_per_keyphrase=100,\n",
    "    final_top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall ê°’ ì €ì¥\n",
    "with open(\"recall_keybert_result.txt\", \"w\") as f:\n",
    "    f.write(f\"Recall: {recall_keybert}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall ê°’ ì €ì¥\n",
    "with open(\"recall_result_test.txt\", \"w\") as f:\n",
    "    f.write(f\"Recall: {recall_dpr}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- custom checking code\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "j_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
